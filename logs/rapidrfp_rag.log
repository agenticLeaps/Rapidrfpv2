2025-11-02 02:00:28,948 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:00:28,948 - root - INFO - Starting Flask API server on http://0.0.0.0:5000
2025-11-02 02:00:46,544 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:00:46,544 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:00:46,589 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 02:00:46,590 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 02:00:46,590 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:00:46,668 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:00:47,453 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/8a2454c0-712f-41fa-b44c-79fbe4b04ad1 "HTTP/1.1 200 OK"
2025-11-02 02:00:52,857 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:00:52,857 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:00:52,878 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:00:52,886 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:00:52,988 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:00:53,764 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/8298f25b-b284-4a35-ac17-f233acf0a111 "HTTP/1.1 200 OK"
2025-11-02 02:05:25,216 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/api/routes.py', reloading
2025-11-02 02:05:25,320 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:05:31,232 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:05:31,233 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:05:31,254 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:05:31,262 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:05:31,366 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:05:32,174 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/cb1119b1-01df-4061-988c-1f480f5bd839 "HTTP/1.1 200 OK"
2025-11-02 02:10:15,357 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 02:10:15] "[31m[1mGET /api/upload/document HTTP/1.1[0m" 405 -
2025-11-02 02:10:20,398 - src.api.routes - INFO - File uploaded to data/raw/37fc8e0f-6c7b-4178-82ca-c89464d0e363_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:10:20,398 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/37fc8e0f-6c7b-4178-82ca-c89464d0e363_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:10:20,749 - src.document_processing.document_loader - INFO - Loaded document data/raw/37fc8e0f-6c7b-4178-82ca-c89464d0e363_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 02:10:20,749 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 02:10:20,749 - src.llm.llm_service - ERROR - Error extracting entities: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:20,749 - src.llm.llm_service - ERROR - Error extracting semantic units: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:20,749 - src.llm.llm_service - ERROR - Error extracting entities: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:20,749 - src.llm.llm_service - ERROR - Error extracting semantic units: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:20,749 - src.llm.llm_service - ERROR - Error extracting entities: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:20,750 - src.llm.llm_service - ERROR - Error extracting semantic units: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:20,750 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 3 successful, 0 failed
2025-11-02 02:10:20,750 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-02 02:10:20,751 - src.document_processing.indexing_pipeline - ERROR - Error in Phase II augmentation: not implemented for multigraph type
2025-11-02 02:10:20,752 - src.api.routes - INFO - Cleaned up temporary file data/raw/37fc8e0f-6c7b-4178-82ca-c89464d0e363_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:10:20,752 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 02:10:20] "[35m[1mPOST /api/upload/document HTTP/1.1[0m" 500 -
2025-11-02 02:10:38,217 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:10:38,218 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:10:38,257 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 02:10:38,257 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 02:10:38,258 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:10:38,355 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:10:39,154 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/4cf112cb-f55d-43ef-94f2-a7ae43554d0d "HTTP/1.1 200 OK"
2025-11-02 02:10:44,269 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:10:44,269 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:10:44,289 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:10:44,299 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:10:44,322 - src.api.routes - INFO - File uploaded to data/raw/2bf2780d-5d5b-4491-9809-138de801afa4_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:10:44,322 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/2bf2780d-5d5b-4491-9809-138de801afa4_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:10:44,373 - src.document_processing.document_loader - INFO - Loaded document data/raw/2bf2780d-5d5b-4491-9809-138de801afa4_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 02:10:44,373 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 02:10:44,373 - src.llm.llm_service - ERROR - Error extracting entities: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:44,373 - src.llm.llm_service - ERROR - Error extracting semantic units: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:44,373 - src.llm.llm_service - ERROR - Error extracting entities: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:44,373 - src.llm.llm_service - ERROR - Error extracting semantic units: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:44,373 - src.llm.llm_service - ERROR - Error extracting entities: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:44,373 - src.llm.llm_service - ERROR - Error extracting semantic units: Cannot find a function with `api_name`: /predict.
2025-11-02 02:10:44,373 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 3 successful, 0 failed
2025-11-02 02:10:44,373 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-02 02:10:44,374 - src.document_processing.indexing_pipeline - ERROR - Error in Phase II augmentation: not implemented for multigraph type
2025-11-02 02:10:44,374 - src.api.routes - INFO - Cleaned up temporary file data/raw/2bf2780d-5d5b-4491-9809-138de801afa4_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:10:44,374 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 02:10:44] "[35m[1mPOST /api/upload/document HTTP/1.1[0m" 500 -
2025-11-02 02:10:44,393 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:10:45,176 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/aecb617c-c35e-45f9-a73c-b8a3db494b50 "HTTP/1.1 200 OK"
2025-11-02 02:11:11,219 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:11:11,329 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:11:17,015 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:11:17,015 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:11:17,039 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:11:17,046 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:11:17,176 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:11:17,939 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/b9f30ef3-c17b-4a1c-bb8b-302560db1428 "HTTP/1.1 200 OK"
2025-11-02 02:11:21,168 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:11:21,228 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:11:27,170 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:11:27,170 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:11:27,191 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:11:27,198 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:11:27,306 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:11:28,079 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/5b83469f-6e44-4bcc-a2ba-7107147401fa "HTTP/1.1 200 OK"
2025-11-02 02:11:29,271 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:11:29,373 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:11:35,352 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:11:35,353 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:11:35,373 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:11:35,381 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:11:35,495 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:11:36,259 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/7a0f116a-6390-475e-ad77-e112a124c786 "HTTP/1.1 200 OK"
2025-11-02 02:11:38,496 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:11:38,574 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:11:44,334 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:11:44,334 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:11:44,358 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:11:44,366 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:11:44,491 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:11:45,245 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/127bcd4c-1fc9-4303-b2fb-5f78853021cd "HTTP/1.1 200 OK"
2025-11-02 02:11:53,690 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/graph/graph_manager.py', reloading
2025-11-02 02:11:53,759 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:12:00,966 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:12:00,967 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:12:00,987 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:12:00,992 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:12:01,104 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:12:01,892 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/c12d15ce-6745-4493-a724-4d4d7cbcc6ce "HTTP/1.1 200 OK"
2025-11-02 02:12:07,190 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:12:07,297 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:12:13,168 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:12:13,168 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:12:13,191 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:12:13,199 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:12:13,303 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:12:14,080 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/4ee5848b-38e5-4036-862d-66a55b1a2a3f "HTTP/1.1 200 OK"
2025-11-02 02:12:45,953 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:12:45,955 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:12:45,986 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 02:12:45,986 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 02:12:45,987 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:12:46,093 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:12:46,869 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/b4b12789-6098-4af4-945e-29ff4380776c "HTTP/1.1 200 OK"
2025-11-02 02:12:52,483 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:12:52,484 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:12:52,507 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:12:52,517 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:12:52,538 - src.api.routes - INFO - File uploaded to data/raw/1bab7a0b-f039-4f07-afe9-ea0b69358e61_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:12:52,538 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/1bab7a0b-f039-4f07-afe9-ea0b69358e61_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:12:52,571 - src.document_processing.document_loader - INFO - Loaded document data/raw/1bab7a0b-f039-4f07-afe9-ea0b69358e61_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 02:12:52,571 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 02:12:52,571 - src.llm.llm_service - ERROR - Error extracting entities: Cannot find a function with `api_name`: /predict.
2025-11-02 02:12:52,571 - src.llm.llm_service - ERROR - Error extracting semantic units: Cannot find a function with `api_name`: /predict.
2025-11-02 02:12:52,571 - src.llm.llm_service - ERROR - Error extracting entities: Cannot find a function with `api_name`: /predict.
2025-11-02 02:12:52,571 - src.llm.llm_service - ERROR - Error extracting semantic units: Cannot find a function with `api_name`: /predict.
2025-11-02 02:12:52,571 - src.llm.llm_service - ERROR - Error extracting entities: Cannot find a function with `api_name`: /predict.
2025-11-02 02:12:52,571 - src.llm.llm_service - ERROR - Error extracting semantic units: Cannot find a function with `api_name`: /predict.
2025-11-02 02:12:52,571 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 3 successful, 0 failed
2025-11-02 02:12:52,571 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-02 02:12:52,572 - src.document_processing.indexing_pipeline - ERROR - Error in Phase II augmentation: float division by zero
2025-11-02 02:12:52,572 - src.api.routes - INFO - Cleaned up temporary file data/raw/1bab7a0b-f039-4f07-afe9-ea0b69358e61_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:12:52,572 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 02:12:52] "[35m[1mPOST /api/upload/document HTTP/1.1[0m" 500 -
2025-11-02 02:12:52,629 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:12:53,432 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/5c707e64-5a00-4660-99a4-3521fec4ec22 "HTTP/1.1 200 OK"
2025-11-02 02:13:38,058 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/graph/graph_manager.py', reloading
2025-11-02 02:13:38,149 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:13:44,421 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:13:44,421 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:13:44,447 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:13:44,455 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:13:44,553 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:13:45,339 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/36591f8f-ca80-4ad1-9251-6a0fe714e4b5 "HTTP/1.1 200 OK"
2025-11-02 02:13:46,540 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:13:46,619 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:13:52,314 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:13:52,315 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:13:52,335 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:13:52,340 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:13:52,435 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:13:53,227 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/cf65b273-7488-49c3-a7e4-9bd9e3e588af "HTTP/1.1 200 OK"
2025-11-02 02:16:04,605 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:16:04,701 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:16:10,497 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:16:10,498 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:16:10,529 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:16:10,542 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:16:10,625 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:16:11,377 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/4777a83c-a401-4d2e-8055-a2133086c25d "HTTP/1.1 200 OK"
2025-11-02 02:16:13,667 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:16:13,788 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:16:20,490 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:16:20,490 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:16:20,519 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:16:20,524 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:16:20,630 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:16:21,444 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/20a55eb3-404d-4d47-9990-b3fa0beed566 "HTTP/1.1 200 OK"
2025-11-02 02:16:23,633 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:16:23,704 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:16:29,927 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:16:29,928 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:16:29,949 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:16:29,955 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:16:30,062 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:16:30,881 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/55a026bd-ce78-4d45-8603-d4fb4e2dc2b9 "HTTP/1.1 200 OK"
2025-11-02 02:16:33,082 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:16:33,172 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:16:39,116 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:16:39,117 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:16:39,138 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:16:39,144 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:16:39,272 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:16:40,045 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/1071b6eb-065e-43f0-9a89-f6b121dfc364 "HTTP/1.1 200 OK"
2025-11-02 02:16:43,309 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:16:43,392 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:16:49,192 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:16:49,192 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:16:49,218 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:16:49,223 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:16:49,340 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:16:50,107 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/74d9c63e-b91f-406d-9e8e-1cf1b5856bee "HTTP/1.1 200 OK"
2025-11-02 02:16:57,455 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:16:57,536 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:17:03,341 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:17:03,342 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:17:03,362 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:17:03,368 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:17:03,478 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:17:04,270 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/d5f91ad4-e86f-479c-ac24-eeec0ee1ed60 "HTTP/1.1 200 OK"
2025-11-02 02:18:25,835 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:18:25,836 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:18:25,860 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 02:18:25,860 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 02:18:25,861 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:18:25,972 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:18:26,734 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/9fe1ef45-aa28-4fd4-95a7-0df43bd024ba "HTTP/1.1 200 OK"
2025-11-02 02:18:31,640 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:18:31,641 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:18:31,667 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:18:31,673 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:18:31,821 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:18:32,653 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/f6f33def-e252-454e-9ab7-bd501ab7e556 "HTTP/1.1 200 OK"
2025-11-02 02:18:52,013 - src.api.routes - INFO - File uploaded to data/raw/fbd9e837-1df3-46f5-84a7-cfa5a5c80bd6_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:18:52,014 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/fbd9e837-1df3-46f5-84a7-cfa5a5c80bd6_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:18:52,068 - src.document_processing.document_loader - INFO - Loaded document data/raw/fbd9e837-1df3-46f5-84a7-cfa5a5c80bd6_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 02:18:52,068 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 02:18:53,105 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:18:54,089 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:19:20,358 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:19:21,322 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:19:30,587 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:19:31,589 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:20:12,850 - src.llm.llm_service - WARNING - Failed to parse relationships JSON
2025-11-02 02:20:13,837 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:20:14,832 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:20:28,792 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:20:29,848 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:20:46,574 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:20:47,562 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:20:47,592 - src.llm.llm_service - ERROR - Error extracting relationships: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:58:06
2025-11-02 02:20:48,557 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:20:49,562 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:20:49,578 - src.llm.llm_service - ERROR - Error extracting entities: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:58:04
2025-11-02 02:20:50,516 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:20:51,541 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:20:51,594 - src.llm.llm_service - ERROR - Error extracting semantic units: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:58:02
2025-11-02 02:20:51,595 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 3 successful, 0 failed
2025-11-02 02:20:51,595 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-02 02:20:51,606 - src.document_processing.indexing_pipeline - INFO - Identified 5 important entities
2025-11-02 02:20:52,550 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:20:53,554 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:20:53,573 - src.llm.llm_service - ERROR - Error generating entity attributes: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:58:00
2025-11-02 02:20:54,538 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:20:55,518 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:20:55,562 - src.llm.llm_service - ERROR - Error generating entity attributes: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:58
2025-11-02 02:20:56,549 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:20:57,533 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:20:57,575 - src.llm.llm_service - ERROR - Error generating entity attributes: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:56
2025-11-02 02:20:58,533 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:20:59,509 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:20:59,560 - src.llm.llm_service - ERROR - Error generating entity attributes: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:54
2025-11-02 02:21:00,572 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:01,574 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:01,594 - src.llm.llm_service - ERROR - Error generating entity attributes: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:52
2025-11-02 02:21:01,595 - src.document_processing.indexing_pipeline - INFO - Created 5 attribute nodes
2025-11-02 02:21:03,531 - src.graph.graph_manager - INFO - Detected 8 communities
2025-11-02 02:21:04,521 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:05,460 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:05,489 - src.llm.llm_service - ERROR - Error generating community summary: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:48
2025-11-02 02:21:06,538 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:07,467 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:07,500 - src.llm.llm_service - ERROR - Error generating community overview: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:46
2025-11-02 02:21:08,506 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:09,499 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:09,528 - src.llm.llm_service - ERROR - Error generating community summary: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:44
2025-11-02 02:21:10,471 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:11,411 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:11,435 - src.llm.llm_service - ERROR - Error generating community overview: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:42
2025-11-02 02:21:12,396 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:13,375 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:13,409 - src.llm.llm_service - ERROR - Error generating community summary: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:40
2025-11-02 02:21:14,404 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:15,376 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:15,426 - src.llm.llm_service - ERROR - Error generating community overview: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:38
2025-11-02 02:21:16,390 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:17,363 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:17,414 - src.llm.llm_service - ERROR - Error generating community summary: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:36
2025-11-02 02:21:18,398 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:19,366 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:19,410 - src.llm.llm_service - ERROR - Error generating community overview: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:34
2025-11-02 02:21:20,405 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:21,395 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:21,410 - src.llm.llm_service - ERROR - Error generating community summary: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:32
2025-11-02 02:21:22,395 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:23,341 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:23,348 - src.llm.llm_service - ERROR - Error generating community overview: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:30
2025-11-02 02:21:24,311 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:25,318 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:25,327 - src.llm.llm_service - ERROR - Error generating community summary: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:28
2025-11-02 02:21:26,261 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:27,291 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:27,327 - src.llm.llm_service - ERROR - Error generating community overview: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:26
2025-11-02 02:21:28,336 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:29,337 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:29,358 - src.llm.llm_service - ERROR - Error generating community summary: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:24
2025-11-02 02:21:30,303 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:31,308 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:31,331 - src.llm.llm_service - ERROR - Error generating community overview: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:22
2025-11-02 02:21:32,278 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:33,261 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:33,298 - src.llm.llm_service - ERROR - Error generating community summary: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:20
2025-11-02 02:21:34,268 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:21:35,213 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=c50ab287-a6b8-450b-a57f-6e9f35fcfb1f "HTTP/1.1 200 OK"
2025-11-02 02:21:35,240 - src.llm.llm_service - ERROR - Error generating community overview: You have exceeded your GPU quota (60s requested vs. 51s left). Try again in 23:57:18
2025-11-02 02:21:35,241 - src.document_processing.indexing_pipeline - INFO - Created 8 high-level nodes and 8 overview nodes
2025-11-02 02:21:35,242 - src.document_processing.indexing_pipeline - INFO - Document indexing completed in 163.23 seconds
2025-11-02 02:21:35,243 - src.graph.graph_manager - INFO - Graph saved to data/processed/graph.gpickle
2025-11-02 02:21:35,246 - src.api.routes - INFO - Cleaned up temporary file data/raw/fbd9e837-1df3-46f5-84a7-cfa5a5c80bd6_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:21:35,248 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 02:21:35] "POST /api/upload/document HTTP/1.1" 200 -
2025-11-02 02:22:19,075 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:22:19,159 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:22:25,399 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:22:25,400 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:22:25,424 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:22:25,431 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:22:25,565 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:22:26,365 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/6034d928-76f8-4824-87cd-fdeb14f24d64 "HTTP/1.1 200 OK"
2025-11-02 02:22:29,607 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:22:29,675 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:22:35,603 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:22:35,603 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:22:35,623 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:22:35,628 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:22:35,758 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:22:36,574 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/1398c573-9459-49fe-af37-38f55f966a94 "HTTP/1.1 200 OK"
2025-11-02 02:22:45,976 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 02:22:46,063 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:22:52,176 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:22:52,176 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:22:52,197 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:22:52,205 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:22:52,328 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:22:53,150 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/0a3c6e69-f5ac-4a31-ac4b-61e1f2b65c56 "HTTP/1.1 200 OK"
2025-11-02 02:23:29,115 - src.api.routes - ERROR - Error in get_graph_stats: Connectivity is undefined for the null graph.
2025-11-02 02:23:29,116 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 02:23:29] "[35m[1mGET /api/graph/stats HTTP/1.1[0m" 500 -
2025-11-02 02:23:53,857 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:23:53,857 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:23:53,888 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 02:23:53,888 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 02:23:53,889 - werkzeug - INFO -  * Restarting with stat
2025-11-02 02:23:53,995 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:23:54,810 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/27bcf4f0-7fd2-4d44-a8b2-c72a100bfd7b "HTTP/1.1 200 OK"
2025-11-02 02:23:59,620 - root - INFO - Starting RapidRFP RAG System
2025-11-02 02:23:59,620 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 02:23:59,646 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 02:23:59,658 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-02 02:23:59,686 - src.api.routes - INFO - File uploaded to data/raw/ce924e92-821f-4bc6-bfe6-570a309d8763_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:23:59,686 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/ce924e92-821f-4bc6-bfe6-570a309d8763_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 02:23:59,720 - src.document_processing.document_loader - INFO - Loaded document data/raw/ce924e92-821f-4bc6-bfe6-570a309d8763_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 02:23:59,720 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 02:23:59,768 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 02:24:00,547 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/a821aa81-47b4-4271-8c77-9262a8411c4b "HTTP/1.1 200 OK"
2025-11-02 02:24:00,676 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 02:24:01,675 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=72a1a110-1e5a-4637-96a1-44ea79fec499 "HTTP/1.1 200 OK"
2025-11-02 02:24:01,711 - src.llm.llm_service - WARNING - GPU quota exceeded, waiting 300 seconds before retry 1
2025-11-02 09:26:16,027 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:26:16,027 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:26:16,064 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 09:26:16,064 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 09:26:16,065 - werkzeug - INFO -  * Restarting with stat
2025-11-02 09:26:21,905 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:26:21,905 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:26:21,927 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 09:26:21,935 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-02 09:26:22,085 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 09:26:22,804 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/b85ef112-2e1c-4e26-933a-138db361ad3a "HTTP/1.1 200 OK"
2025-11-02 09:26:23,325 - src.api.routes - INFO - File uploaded to data/raw/f44991e1-a663-4edd-b66e-71a8f343b895_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:26:23,325 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/f44991e1-a663-4edd-b66e-71a8f343b895_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:26:23,373 - src.document_processing.document_loader - INFO - Loaded document data/raw/f44991e1-a663-4edd-b66e-71a8f343b895_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 09:26:23,373 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 09:26:24,385 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:26:25,362 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=23b96a52-3004-498b-ad63-3505ee97f038 "HTTP/1.1 200 OK"
2025-11-02 09:26:25,397 - src.llm.llm_service - WARNING - GPU quota exceeded, waiting 300 seconds before retry 1
2025-11-02 09:28:58,524 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:28:58,524 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:28:58,554 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 09:28:58,554 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 09:28:58,555 - werkzeug - INFO -  * Restarting with stat
2025-11-02 09:28:58,666 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 09:28:59,418 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/36cb131c-e9b1-41fe-9c63-e111d86eef09 "HTTP/1.1 200 OK"
2025-11-02 09:29:04,226 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:29:04,226 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:29:04,246 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 09:29:04,253 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-02 09:29:04,357 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 09:29:05,169 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/b83e6d47-6d44-49df-bcef-e5ad2d821e6a "HTTP/1.1 200 OK"
2025-11-02 09:29:08,959 - src.api.routes - INFO - File uploaded to data/raw/f05904e3-c453-4b52-9507-2fd0de359717_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:29:08,959 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/f05904e3-c453-4b52-9507-2fd0de359717_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:29:09,002 - src.document_processing.document_loader - INFO - Loaded document data/raw/f05904e3-c453-4b52-9507-2fd0de359717_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 09:29:09,002 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 09:29:09,949 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:29:10,953 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=b834a403-46e7-47a4-9898-c94a8f89fbd9 "HTTP/1.1 200 OK"
2025-11-02 09:29:10,974 - src.llm.llm_service - WARNING - GPU quota exceeded, waiting 300 seconds before retry 1
2025-11-02 09:38:42,461 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:38:42,461 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:38:42,488 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 09:38:42,488 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 09:38:42,488 - werkzeug - INFO -  * Restarting with stat
2025-11-02 09:38:42,585 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 09:38:43,381 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/dfc33be2-8c89-4c90-9461-f36876ff5f1f "HTTP/1.1 200 OK"
2025-11-02 09:38:48,085 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:38:48,085 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:38:48,102 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 09:38:48,109 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-02 09:38:48,144 - src.api.routes - INFO - File uploaded to data/raw/64e83d39-053e-41ad-933a-c9e511f1e01b_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:38:48,144 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/64e83d39-053e-41ad-933a-c9e511f1e01b_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:38:48,178 - src.document_processing.document_loader - INFO - Loaded document data/raw/64e83d39-053e-41ad-933a-c9e511f1e01b_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 09:38:48,178 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 09:38:48,272 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 09:38:48,987 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/47415161-6818-4876-ac04-604ad7eeaba9 "HTTP/1.1 200 OK"
2025-11-02 09:38:49,151 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:38:50,142 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:38:50,164 - src.llm.llm_service - ERROR - Error extracting entities: 'tuple' object has no attribute 'find'
2025-11-02 09:38:51,158 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:38:52,150 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:38:52,168 - src.llm.llm_service - ERROR - Error extracting semantic units: 'tuple' object has no attribute 'find'
2025-11-02 09:38:53,102 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:38:54,102 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:38:54,157 - src.llm.llm_service - ERROR - Error extracting entities: 'tuple' object has no attribute 'find'
2025-11-02 09:38:55,117 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:38:56,065 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:38:56,083 - src.llm.llm_service - ERROR - Error extracting semantic units: 'tuple' object has no attribute 'find'
2025-11-02 09:38:57,141 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:38:58,136 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:38:58,161 - src.llm.llm_service - ERROR - Error extracting entities: 'tuple' object has no attribute 'find'
2025-11-02 09:38:59,180 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:39:00,171 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:39:00,204 - src.llm.llm_service - ERROR - Error extracting semantic units: 'tuple' object has no attribute 'find'
2025-11-02 09:39:00,204 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 3 successful, 0 failed
2025-11-02 09:39:00,204 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-02 09:39:00,206 - src.document_processing.indexing_pipeline - INFO - Identified 0 important entities
2025-11-02 09:39:00,206 - src.document_processing.indexing_pipeline - INFO - Created 0 attribute nodes
2025-11-02 09:39:00,374 - src.graph.graph_manager - INFO - Detected 3 communities
2025-11-02 09:39:01,319 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:39:02,327 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:39:02,341 - src.llm.llm_service - ERROR - Error generating community summary: 'tuple' object has no attribute 'strip'
2025-11-02 09:39:03,346 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:39:04,346 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:39:04,364 - src.llm.llm_service - ERROR - Error generating community overview: 'tuple' object has no attribute 'strip'
2025-11-02 09:39:05,317 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:39:06,322 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:39:06,335 - src.llm.llm_service - ERROR - Error generating community summary: 'tuple' object has no attribute 'strip'
2025-11-02 09:39:07,285 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:39:08,290 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:39:08,292 - src.llm.llm_service - ERROR - Error generating community overview: 'tuple' object has no attribute 'strip'
2025-11-02 09:39:09,284 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:39:10,245 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:39:10,252 - src.llm.llm_service - ERROR - Error generating community summary: 'tuple' object has no attribute 'strip'
2025-11-02 09:39:11,215 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-02 09:39:12,307 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen2-5.hf.space/gradio_api/queue/data?session_hash=690e45b7-66c6-4014-9631-4a47f706a8b4 "HTTP/1.1 200 OK"
2025-11-02 09:39:12,338 - src.llm.llm_service - ERROR - Error generating community overview: 'tuple' object has no attribute 'strip'
2025-11-02 09:39:12,338 - src.document_processing.indexing_pipeline - INFO - Created 3 high-level nodes and 3 overview nodes
2025-11-02 09:39:12,339 - src.document_processing.indexing_pipeline - INFO - Document indexing completed in 24.19 seconds
2025-11-02 09:39:12,342 - src.graph.graph_manager - INFO - Graph saved to data/processed/graph.gpickle
2025-11-02 09:39:12,345 - src.api.routes - INFO - Cleaned up temporary file data/raw/64e83d39-053e-41ad-933a-c9e511f1e01b_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:39:12,346 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:39:12] "POST /api/upload/document HTTP/1.1" 200 -
2025-11-02 09:45:00,326 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 09:45:00,438 - werkzeug - INFO -  * Restarting with stat
2025-11-02 09:48:22,721 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:48:22,722 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:48:22,756 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 09:48:22,756 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 09:48:22,757 - werkzeug - INFO -  * Restarting with stat
2025-11-02 09:48:23,145 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:48:23,145 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:48:23,158 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 09:48:23,165 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-02 09:48:27,203 - src.api.routes - INFO - File uploaded to data/raw/f8643d97-f0ef-4dcb-be63-2d2c07574a73_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:48:27,203 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/f8643d97-f0ef-4dcb-be63-2d2c07574a73_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:48:27,255 - src.document_processing.document_loader - INFO - Loaded document data/raw/f8643d97-f0ef-4dcb-be63-2d2c07574a73_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 09:48:27,255 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 09:48:30,457 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:48:33,591 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:48:38,496 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:48:41,105 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:48:44,421 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:48:49,779 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:48:52,796 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:48:54,139 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:48:56,255 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:48:56,261 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 3 successful, 0 failed
2025-11-02 09:48:56,261 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-02 09:48:56,286 - src.document_processing.indexing_pipeline - INFO - Identified 10 important entities
2025-11-02 09:48:59,067 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:03,185 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:05,483 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:09,110 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:11,255 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:14,239 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:16,418 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:18,875 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:21,997 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:24,083 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:24,098 - src.document_processing.indexing_pipeline - INFO - Created 10 attribute nodes
2025-11-02 09:49:24,187 - src.graph.graph_manager - INFO - Detected 6 communities
2025-11-02 09:49:25,666 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:26,281 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:27,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:28,558 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:30,043 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:30,841 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:33,110 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:33,893 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:35,971 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:36,604 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:38,759 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:39,822 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:49:39,829 - src.document_processing.indexing_pipeline - INFO - Created 6 high-level nodes and 6 overview nodes
2025-11-02 09:49:39,830 - src.document_processing.indexing_pipeline - INFO - Document indexing completed in 72.63 seconds
2025-11-02 09:49:39,832 - src.graph.graph_manager - INFO - Graph saved to data/processed/graph.gpickle
2025-11-02 09:49:39,832 - src.api.routes - INFO - Cleaned up temporary file data/raw/f8643d97-f0ef-4dcb-be63-2d2c07574a73_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:49:39,834 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:49:39] "POST /api/upload/document HTTP/1.1" 200 -
2025-11-02 09:50:12,782 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:50:12] "GET /api/graph/stats HTTP/1.1" 200 -
2025-11-02 09:51:19,810 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:51:19] "POST /api/search/entities HTTP/1.1" 200 -
2025-11-02 09:51:36,803 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:51:36] "POST /api/search/entities HTTP/1.1" 200 -
2025-11-02 09:52:02,377 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:52:02] "GET /api/graph/nodes/N HTTP/1.1" 200 -
2025-11-02 09:52:16,046 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:52:16] "GET /api/graph/nodes/A HTTP/1.1" 200 -
2025-11-02 09:52:29,806 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:52:29] "GET /api/graph/nodes/N HTTP/1.1" 200 -
2025-11-02 09:52:49,148 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:52:49] "GET /api/graph/node/N_f4b70929-892f-46c7-88e2-3a7c6bb0b877_0 HTTP/1.1" 200 -
2025-11-02 09:53:11,342 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:53:11] "GET /api/graph/communities HTTP/1.1" 200 -
2025-11-02 09:53:40,603 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:53:40] "GET /api/graph/important-entities HTTP/1.1" 200 -
2025-11-02 09:56:24,009 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:56:24] "POST /api/search/entities HTTP/1.1" 200 -
2025-11-02 09:56:31,207 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:56:31] "POST /api/search/entities HTTP/1.1" 200 -
2025-11-02 09:58:35,563 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 09:58:35,687 - werkzeug - INFO -  * Restarting with stat
2025-11-02 09:58:36,329 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:58:36,329 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:58:36,347 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 09:58:36,352 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-02 09:58:42,591 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/llm/llm_service.py', reloading
2025-11-02 09:58:42,693 - werkzeug - INFO -  * Restarting with stat
2025-11-02 09:59:32,362 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:59:32,362 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:59:32,393 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 09:59:32,393 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 09:59:32,394 - werkzeug - INFO -  * Restarting with stat
2025-11-02 09:59:32,496 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 09:59:33,278 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/ba49535b-1971-4492-8eae-a8b267ac4d41 "HTTP/1.1 200 OK"
2025-11-02 09:59:35,522 - root - INFO - Starting RapidRFP RAG System
2025-11-02 09:59:35,523 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 09:59:35,539 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 09:59:35,545 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-02 09:59:35,652 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 09:59:36,438 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/5505b7eb-8772-466d-8a41-dc24c877eb21 "HTTP/1.1 200 OK"
2025-11-02 09:59:39,376 - src.api.routes - INFO - File uploaded to data/raw/9b5886ba-1187-4164-8dcd-a06b73949051_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:59:39,376 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/9b5886ba-1187-4164-8dcd-a06b73949051_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:59:39,424 - src.document_processing.document_loader - INFO - Loaded document data/raw/9b5886ba-1187-4164-8dcd-a06b73949051_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 09:59:39,424 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 09:59:41,470 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:59:44,352 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:59:46,991 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:59:48,619 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:59:50,284 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:59:52,934 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:59:53,946 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:59:55,737 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:59:57,640 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:59:57,642 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 3 successful, 0 failed
2025-11-02 09:59:57,642 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-02 09:59:57,672 - src.document_processing.indexing_pipeline - ERROR - Error in Phase II augmentation: Input graph has self loops which is not permitted; Consider using G.remove_edges_from(nx.selfloop_edges(G)).
2025-11-02 09:59:57,672 - src.api.routes - INFO - Cleaned up temporary file data/raw/9b5886ba-1187-4164-8dcd-a06b73949051_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 09:59:57,673 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 09:59:57] "[35m[1mPOST /api/upload/document HTTP/1.1[0m" 500 -
2025-11-02 10:00:49,534 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/graph/graph_manager.py', reloading
2025-11-02 10:00:49,699 - werkzeug - INFO -  * Restarting with stat
2025-11-02 10:00:53,054 - root - INFO - Starting RapidRFP RAG System
2025-11-02 10:00:53,054 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 10:00:53,075 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 10:00:53,082 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-02 10:00:53,191 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 10:00:54,006 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/aa830bb9-46cd-4948-a059-67a217c7e02a "HTTP/1.1 200 OK"
2025-11-02 10:00:58,288 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/graph/graph_manager.py', reloading
2025-11-02 10:00:58,465 - werkzeug - INFO -  * Restarting with stat
2025-11-02 10:01:02,217 - root - INFO - Starting RapidRFP RAG System
2025-11-02 10:01:02,217 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 10:01:02,242 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 10:01:02,250 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-02 10:01:02,357 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 10:01:03,124 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/8ba7d40f-ab32-4b95-a08c-e90be5e97460 "HTTP/1.1 200 OK"
2025-11-02 10:02:16,229 - root - INFO - Starting RapidRFP RAG System
2025-11-02 10:02:16,229 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 10:02:16,257 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-02 10:02:16,257 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-02 10:02:16,257 - werkzeug - INFO -  * Restarting with stat
2025-11-02 10:02:16,759 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 10:02:17,160 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/1f47905e-dea6-4c31-beb2-cff3cda847f5 "HTTP/1.1 200 OK"
2025-11-02 10:02:19,195 - root - INFO - Starting RapidRFP RAG System
2025-11-02 10:02:19,195 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-02 10:02:19,228 - werkzeug - WARNING -  * Debugger is active!
2025-11-02 10:02:19,245 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-02 10:02:19,268 - src.api.routes - INFO - File uploaded to data/raw/930bcbed-87a0-486b-ac5b-64fed4afddff_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 10:02:19,268 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/930bcbed-87a0-486b-ac5b-64fed4afddff_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 10:02:19,301 - src.document_processing.document_loader - INFO - Loaded document data/raw/930bcbed-87a0-486b-ac5b-64fed4afddff_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-02 10:02:19,301 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-02 10:02:19,331 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-02 10:02:20,112 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/de73dadf-4551-401d-93a4-9ef50b7aa113 "HTTP/1.1 200 OK"
2025-11-02 10:02:21,606 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:24,272 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:27,620 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:29,430 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:37,067 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:39,563 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:41,006 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:42,378 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:44,688 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:44,695 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 3 successful, 0 failed
2025-11-02 10:02:44,695 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-02 10:02:44,722 - src.document_processing.indexing_pipeline - INFO - Identified 10 important entities
2025-11-02 10:02:48,196 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:51,564 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:53,655 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:57,366 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:02:59,208 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:02,750 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:08,905 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:12,636 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:14,946 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:18,296 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:18,307 - src.document_processing.indexing_pipeline - INFO - Created 10 attribute nodes
2025-11-02 10:03:18,394 - src.graph.graph_manager - INFO - Detected 6 communities
2025-11-02 10:03:20,991 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:21,649 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:23,247 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:23,902 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:25,190 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:25,756 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:27,895 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:28,571 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:31,041 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:31,680 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:32,980 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:33,621 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 10:03:33,636 - src.document_processing.indexing_pipeline - INFO - Created 6 high-level nodes and 6 overview nodes
2025-11-02 10:03:33,637 - src.document_processing.indexing_pipeline - INFO - Document indexing completed in 74.37 seconds
2025-11-02 10:03:33,639 - src.graph.graph_manager - INFO - Graph saved to data/processed/graph.gpickle
2025-11-02 10:03:33,640 - src.api.routes - INFO - Cleaned up temporary file data/raw/930bcbed-87a0-486b-ac5b-64fed4afddff_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-02 10:03:33,641 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 10:03:33] "POST /api/upload/document HTTP/1.1" 200 -
2025-11-02 10:03:55,431 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 10:03:55] "POST /api/search/entities HTTP/1.1" 200 -
2025-11-02 10:03:58,297 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 10:03:58] "GET /api/graph/important-entities HTTP/1.1" 200 -
2025-11-02 10:04:19,929 - werkzeug - INFO - 127.0.0.1 - - [02/Nov/2025 10:04:19] "POST /api/search/entities HTTP/1.1" 200 -
2025-11-09 17:21:05,060 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:21:05,061 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:21:05,131 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-09 17:21:05,131 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-09 17:21:05,132 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:21:05,283 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:21:06,050 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/50f8c0cf-bdbb-457f-ae5f-ddaeecafcedf "HTTP/1.1 200 OK"
2025-11-09 17:21:11,251 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:21:11,251 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:21:11,280 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:21:11,295 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:21:11,481 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:21:12,220 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/cbc95c93-6ad4-4aea-bbf3-bbed940582c0 "HTTP/1.1 200 OK"
2025-11-09 17:21:39,907 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:21:39] "[33mGET / HTTP/1.1[0m" 404 -
2025-11-09 17:21:40,017 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:21:40] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2025-11-09 17:22:34,962 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:22:34] "[33mGET / HTTP/1.1[0m" 404 -
2025-11-09 17:22:35,056 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:22:35] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2025-11-09 17:24:08,695 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:24:08,696 - root - INFO - Starting Flask API server on http://0.0.0.0:5000
2025-11-09 17:24:42,166 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:24:42,167 - root - INFO - Starting Flask API server on http://0.0.0.0:5000
2025-11-09 17:25:29,334 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:25:29,334 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:25:29,372 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-09 17:25:29,372 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-09 17:25:29,373 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:25:29,561 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:25:30,340 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/1abacd12-469b-4d00-ba5a-602275757351 "HTTP/1.1 200 OK"
2025-11-09 17:25:35,282 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:25:35,282 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:25:35,311 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:25:35,324 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:25:35,376 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:25:35] "GET /health HTTP/1.1" 200 -
2025-11-09 17:25:35,377 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:25:35] "GET /health HTTP/1.1" 200 -
2025-11-09 17:25:35,505 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:25:36,261 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/27b5979c-4d54-40c7-b2ac-885914702876 "HTTP/1.1 200 OK"
2025-11-09 17:27:47,412 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:27:47] "GET /health HTTP/1.1" 200 -
2025-11-09 17:28:59,974 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/config/settings.py', reloading
2025-11-09 17:29:00,241 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:29:06,513 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:29:06,514 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:29:06,542 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:29:06,552 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:29:06,739 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:29:07,516 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/9896f0d3-1141-46a4-ac8d-b1a7e76af611 "HTTP/1.1 200 OK"
2025-11-09 17:29:22,247 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/web_ui.py', reloading
2025-11-09 17:29:22,735 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:29:28,969 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:29:28,969 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:29:28,997 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:29:29,006 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:29:29,194 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:29:29,905 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/7d599201-adbc-4caf-a617-118b0e80a34e "HTTP/1.1 200 OK"
2025-11-09 17:29:39,508 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/web_ui.py', reloading
2025-11-09 17:29:39,725 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:29:46,089 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:29:46,090 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:29:46,148 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:29:46,156 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:29:46,706 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:29:47,068 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/ab2f20a4-c539-4ce3-baaa-14f993781131 "HTTP/1.1 200 OK"
2025-11-09 17:30:12,250 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/run_system.py', reloading
2025-11-09 17:30:12,475 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:30:19,179 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:30:19,179 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:30:19,274 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:30:19,299 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:30:19,404 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:30:20,198 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/5873287d-0b1b-4958-95c3-1045740ad435 "HTTP/1.1 200 OK"
2025-11-09 17:30:31,850 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/run_system.py', reloading
2025-11-09 17:30:32,115 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:30:38,568 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:30:38,568 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:30:38,621 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:30:38,644 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:30:38,800 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:30:39,611 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/841ffb61-b74a-4405-9ee7-8ffa90244a14 "HTTP/1.1 200 OK"
2025-11-09 17:30:46,015 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/run_system.py', reloading
2025-11-09 17:30:46,253 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:30:52,609 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:30:52,610 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:30:52,634 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:30:52,641 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:30:52,841 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:30:53,670 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/f68b52d6-b547-41f9-9b7e-fc86d8fa004f "HTTP/1.1 200 OK"
2025-11-09 17:31:04,206 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/run_system.py', reloading
2025-11-09 17:31:04,442 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:31:10,775 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:31:10,776 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:31:10,804 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:31:10,814 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:31:10,995 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:31:11,898 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/fe0dc513-5a57-4218-9390-53a392279fc7 "HTTP/1.1 200 OK"
2025-11-09 17:31:20,253 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/run_system.py', reloading
2025-11-09 17:31:20,439 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:31:26,568 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:31:26,568 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:31:26,591 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:31:26,599 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:31:27,183 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:31:27,565 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/31fb3b24-2c28-46b7-993c-b53ac46879c4 "HTTP/1.1 200 OK"
2025-11-09 17:31:27,673 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/run_system.py', reloading
2025-11-09 17:31:27,933 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:31:34,051 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:31:34,052 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:31:34,088 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:31:34,097 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:31:34,276 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:31:35,051 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/96d313f7-8542-4214-a2c8-8c5482a51b68 "HTTP/1.1 200 OK"
2025-11-09 17:32:35,584 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:32:35,584 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:32:35,622 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-09 17:32:35,622 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-09 17:32:35,623 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:32:35,809 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:32:36,550 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/04710a7e-d1a2-470b-94cd-bfa54c5b06aa "HTTP/1.1 200 OK"
2025-11-09 17:32:41,748 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:32:41,748 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:32:41,781 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:32:41,791 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:32:41,823 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:32:41] "GET /health HTTP/1.1" 200 -
2025-11-09 17:32:41,823 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:32:41] "GET /health HTTP/1.1" 200 -
2025-11-09 17:32:41,976 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:32:42,781 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/610674c9-b4c2-4180-b5e8-a64c906477d6 "HTTP/1.1 200 OK"
2025-11-09 17:32:42,869 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:32:42] "GET /health HTTP/1.1" 200 -
2025-11-09 17:32:50,473 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:32:50] "GET /health HTTP/1.1" 200 -
2025-11-09 17:32:51,748 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:32:51] "GET /health HTTP/1.1" 200 -
2025-11-09 17:32:51,756 - src.api.routes - INFO - File uploaded to data/raw/30e720a4-db66-47b4-a42b-d28df1f02162_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-09 17:32:51,756 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/30e720a4-db66-47b4-a42b-d28df1f02162_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-09 17:32:51,756 - src.document_processing.llamaparse_service - INFO - Using LlamaParse for data/raw/30e720a4-db66-47b4-a42b-d28df1f02162_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-09 17:32:51,756 - src.document_processing.llamaparse_service - INFO - Submitting parsing job for data/raw/30e720a4-db66-47b4-a42b-d28df1f02162_21pa1a54b8RaghavendraVattikuti (1).pdf...
2025-11-09 17:32:52,303 - src.document_processing.llamaparse_service - ERROR - Job submission error: HTTPSConnectionPool(host='api.cloud.llamaindex.ai', port=443): Max retries exceeded with url: /api/v1/parsing/upload (Caused by SSLError(SSLCertVerificationError(1, "[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'api.cloud.llamaindex.ai'. (_ssl.c:1028)")))
2025-11-09 17:32:52,303 - src.document_processing.llamaparse_service - WARNING - LlamaParse failed for data/raw/30e720a4-db66-47b4-a42b-d28df1f02162_21pa1a54b8RaghavendraVattikuti (1).pdf: HTTPSConnectionPool(host='api.cloud.llamaindex.ai', port=443): Max retries exceeded with url: /api/v1/parsing/upload (Caused by SSLError(SSLCertVerificationError(1, "[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'api.cloud.llamaindex.ai'. (_ssl.c:1028)")))
2025-11-09 17:32:52,303 - src.document_processing.llamaparse_service - INFO - Falling back to original loader for data/raw/30e720a4-db66-47b4-a42b-d28df1f02162_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-09 17:32:52,397 - src.document_processing.document_loader - INFO - Loaded document data/raw/30e720a4-db66-47b4-a42b-d28df1f02162_21pa1a54b8RaghavendraVattikuti (1).pdf: 3 chunks, 1150 tokens
2025-11-09 17:32:52,397 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-09 17:32:55,020 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:00,030 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:03,370 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:06,386 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:08,238 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:12,581 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:14,371 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:16,467 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:18,172 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:18,177 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 3 successful, 0 failed
2025-11-09 17:33:18,178 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-09 17:33:18,209 - src.document_processing.indexing_pipeline - INFO - Identified 9 important entities
2025-11-09 17:33:18,209 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_7b8374c8-df1a-4896-ab8e-1b39b98733df_1: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-09 17:33:18,210 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_7b8374c8-df1a-4896-ab8e-1b39b98733df_18: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-09 17:33:18,210 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_7b8374c8-df1a-4896-ab8e-1b39b98733df_12: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-09 17:33:18,210 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_7b8374c8-df1a-4896-ab8e-1b39b98733df_2: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-09 17:33:18,210 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_7b8374c8-df1a-4896-ab8e-1b39b98733df_17: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-09 17:33:18,210 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_50f4c23f-20b1-479a-ae7c-8be4d3f5fa63_0: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-09 17:33:18,210 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_0cf12a62-62a3-4931-b1ca-70571fec6a23_8: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-09 17:33:18,210 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_50f4c23f-20b1-479a-ae7c-8be4d3f5fa63_5: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-09 17:33:18,210 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_0cf12a62-62a3-4931-b1ca-70571fec6a23_3: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-09 17:33:18,210 - src.document_processing.indexing_pipeline - INFO - Created 0 attribute nodes
2025-11-09 17:33:18,219 - src.graph.graph_manager - INFO - Detected 6 communities
2025-11-09 17:33:21,532 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:22,120 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:24,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:25,665 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:27,164 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:28,344 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:31,398 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:32,144 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:34,788 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:35,865 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:39,966 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:40,695 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 17:33:40,705 - src.document_processing.indexing_pipeline - INFO - Created 6 high-level nodes and 6 overview nodes
2025-11-09 17:33:40,706 - src.document_processing.indexing_pipeline - INFO - Document indexing completed in 48.95 seconds
2025-11-09 17:33:40,709 - src.graph.graph_manager - INFO - Graph saved to data/processed/graph.gpickle
2025-11-09 17:33:40,710 - src.api.routes - INFO - Cleaned up temporary file data/raw/30e720a4-db66-47b4-a42b-d28df1f02162_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-09 17:33:40,712 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:33:40] "POST /api/upload/document HTTP/1.1" 200 -
2025-11-09 17:33:40,728 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:33:40] "GET /api/graph/stats HTTP/1.1" 200 -
2025-11-09 17:34:58,320 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:34:58] "GET /health HTTP/1.1" 200 -
2025-11-09 17:34:58,353 - src.api.routes - INFO - Graph visualizer initialized
2025-11-09 17:34:58,376 - src.graph.graph_manager - INFO - Detected 6 communities
2025-11-09 17:34:58,378 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:34:58] "GET /api/visualization/stats HTTP/1.1" 200 -
2025-11-09 17:35:02,981 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:35:02] "GET /health HTTP/1.1" 200 -
2025-11-09 17:35:02,994 - src.api.routes - ERROR - Error in create_visualization: type object 'Config' has no attribute 'DATA_DIR'
2025-11-09 17:35:02,998 - src.api.routes - ERROR - Traceback (most recent call last):
  File "/Users/raghavendra/new/rapidrfpRag/src/api/routes.py", line 1036, in create_visualization
    output_path = os.path.join(Config.DATA_DIR, "processed", output_filename)
                               ^^^^^^^^^^^^^^^
AttributeError: type object 'Config' has no attribute 'DATA_DIR'

2025-11-09 17:35:02,998 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:35:02] "[35m[1mPOST /api/visualization/create HTTP/1.1[0m" 500 -
2025-11-09 17:35:22,025 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:35:22] "GET /health HTTP/1.1" 200 -
2025-11-09 17:35:22,059 - src.graph.graph_manager - INFO - Detected 6 communities
2025-11-09 17:35:22,059 - werkzeug - INFO - 127.0.0.1 - - [09/Nov/2025 17:35:22] "GET /api/visualization/stats HTTP/1.1" 200 -
2025-11-09 17:35:27,065 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/config/settings.py', reloading
2025-11-09 17:35:27,278 - werkzeug - INFO -  * Restarting with stat
2025-11-09 17:35:34,491 - root - INFO - Starting RapidRFP RAG System
2025-11-09 17:35:34,492 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-09 17:35:34,524 - werkzeug - WARNING -  * Debugger is active!
2025-11-09 17:35:34,536 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-09 17:35:34,717 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-09 17:35:35,470 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/7a10536f-bcb8-4177-aad6-a5719e556812 "HTTP/1.1 200 OK"
2025-11-10 00:38:16,805 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/api/routes.py', reloading
2025-11-10 00:38:17,405 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:38:23,917 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:38:23,918 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:38:24,028 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:38:24,038 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:38:24,146 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:38:24,913 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/4e50c756-54e0-4653-a033-4a709714f57f "HTTP/1.1 200 OK"
2025-11-10 00:38:28,249 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/web_ui.py', reloading
2025-11-10 00:38:28,530 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:38:34,963 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:38:34,964 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:38:35,031 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:38:35,040 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:38:35,197 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:38:35,955 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/99ec1b0c-780c-4351-9993-d996d8e39558 "HTTP/1.1 200 OK"
2025-11-10 00:38:42,349 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/web_ui.py', reloading
2025-11-10 00:38:42,515 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:38:48,570 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:38:48,570 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:38:48,599 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:38:48,609 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:38:48,790 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:38:49,594 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/e58d54a1-72e6-4e79-ac38-abe42610ef6a "HTTP/1.1 200 OK"
2025-11-10 00:39:07,442 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/web_ui.py', reloading
2025-11-10 00:39:07,670 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:39:14,833 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:39:14,833 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:39:14,862 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:39:14,876 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:39:15,071 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:39:15,887 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/7b83a3ae-a76d-46a5-a01a-60c460ce1492 "HTTP/1.1 200 OK"
2025-11-10 00:39:57,817 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:39:57,818 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:39:57,851 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-10 00:39:57,851 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-10 00:39:57,852 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:39:58,040 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:39:58,745 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/a4c201d9-e373-4354-aa7b-b0a985af8647 "HTTP/1.1 200 OK"
2025-11-10 00:40:03,817 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:40:03,818 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:40:03,846 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:40:03,855 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:40:03,890 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:40:03] "GET /health HTTP/1.1" 200 -
2025-11-10 00:40:03,891 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:40:03] "GET /health HTTP/1.1" 200 -
2025-11-10 00:40:04,040 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:40:04,775 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/c6e8f070-9de3-44d0-9c6f-65555e175cc3 "HTTP/1.1 200 OK"
2025-11-10 00:40:05,036 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:40:05] "GET /health HTTP/1.1" 200 -
2025-11-10 00:40:10,791 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:40:10] "GET /health HTTP/1.1" 200 -
2025-11-10 00:40:10,807 - src.api.routes - INFO - Graph visualizer initialized
2025-11-10 00:40:10,808 - src.visualization.graph_visualizer - ERROR - Failed to get visualization stats: Connectivity is undefined for the null graph.
2025-11-10 00:40:10,809 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:40:10] "GET /api/visualization/stats HTTP/1.1" 200 -
2025-11-10 00:40:21,778 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:40:21] "GET /health HTTP/1.1" 200 -
2025-11-10 00:40:23,063 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:40:23] "GET /health HTTP/1.1" 200 -
2025-11-10 00:40:23,071 - src.api.routes - INFO - File uploaded to data/raw/d76b7c96-a151-4405-8077-68aa35c5a266_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:40:23,072 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/d76b7c96-a151-4405-8077-68aa35c5a266_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:40:23,072 - src.document_processing.llamaparse_service - INFO - Using LlamaParse for data/raw/d76b7c96-a151-4405-8077-68aa35c5a266_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:40:23,072 - src.document_processing.llamaparse_service - INFO - Submitting parsing job for data/raw/d76b7c96-a151-4405-8077-68aa35c5a266_21pa1a54b8RaghavendraVattikuti (1).pdf...
2025-11-10 00:40:25,955 - src.document_processing.llamaparse_service - INFO - Job submitted successfully. Job ID: 63d40350-732a-48cd-b2ef-a79c2549ea95
2025-11-10 00:40:25,956 - src.document_processing.llamaparse_service - INFO - Waiting for job 63d40350-732a-48cd-b2ef-a79c2549ea95 to complete...
2025-11-10 00:40:25,956 - src.document_processing.llamaparse_service - INFO - Max wait time: 300s, Poll interval: 5s
2025-11-10 00:44:27,073 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/document_processing/llamaparse_service.py', reloading
2025-11-10 00:44:27,267 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:44:33,472 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:44:33,473 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:44:33,591 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:44:33,601 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:44:33,703 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:44:34,527 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/f215461a-f86a-4cf6-ac74-7f2182cf3051 "HTTP/1.1 200 OK"
2025-11-10 00:44:53,314 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:44:53,315 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:44:53,344 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-10 00:44:53,344 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-10 00:44:53,345 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:44:53,585 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:44:54,344 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/1e6d84e9-f743-4885-910a-9c9fa7282cbf "HTTP/1.1 200 OK"
2025-11-10 00:44:59,612 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:44:59,613 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:44:59,638 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:44:59,647 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:44:59,677 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:44:59] "GET /health HTTP/1.1" 200 -
2025-11-10 00:44:59,678 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:44:59] "GET /health HTTP/1.1" 200 -
2025-11-10 00:44:59,849 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:45:00,647 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/5ebe2e74-b04a-4618-bf93-eb4d86692333 "HTTP/1.1 200 OK"
2025-11-10 00:45:00,681 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:45:00] "GET /health HTTP/1.1" 200 -
2025-11-10 00:45:24,700 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:45:24] "GET /health HTTP/1.1" 200 -
2025-11-10 00:45:26,370 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:45:26] "GET /health HTTP/1.1" 200 -
2025-11-10 00:45:26,380 - src.api.routes - INFO - File uploaded to data/raw/7e77f52d-df2e-41df-8c67-1856bbc9152a_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:45:26,380 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/7e77f52d-df2e-41df-8c67-1856bbc9152a_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:45:26,380 - src.document_processing.llamaparse_service - INFO - Using LlamaParse for data/raw/7e77f52d-df2e-41df-8c67-1856bbc9152a_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:45:26,380 - src.document_processing.llamaparse_service - INFO - Submitting parsing job for data/raw/7e77f52d-df2e-41df-8c67-1856bbc9152a_21pa1a54b8RaghavendraVattikuti (1).pdf...
2025-11-10 00:45:29,193 - src.document_processing.llamaparse_service - INFO - Job submitted successfully. Job ID: 3e526a15-86e8-4d27-8780-11df64c0f816
2025-11-10 00:45:29,195 - src.document_processing.llamaparse_service - INFO - Waiting for job 3e526a15-86e8-4d27-8780-11df64c0f816 to complete...
2025-11-10 00:45:29,195 - src.document_processing.llamaparse_service - INFO - Max wait time: 300s, Poll interval: 5s
2025-11-10 00:45:30,174 - src.document_processing.llamaparse_service - INFO - Job still PENDING, waiting 5s...
2025-11-10 00:45:35,978 - src.document_processing.llamaparse_service - INFO - Job 3e526a15-86e8-4d27-8780-11df64c0f816 completed successfully!
2025-11-10 00:45:37,024 - src.document_processing.llamaparse_service - INFO - Retrieved result for job 3e526a15-86e8-4d27-8780-11df64c0f816
2025-11-10 00:45:37,027 - src.document_processing.llamaparse_service - INFO - Successfully parsed data/raw/7e77f52d-df2e-41df-8c67-1856bbc9152a_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:45:37,027 - src.document_processing.llamaparse_service - INFO - Found 1 document(s)
2025-11-10 00:45:37,027 - src.document_processing.llamaparse_service - INFO - Converted LlamaParse result: 2 chunks, 697 tokens
2025-11-10 00:45:37,028 - src.document_processing.llamaparse_service - INFO - Successfully loaded data/raw/7e77f52d-df2e-41df-8c67-1856bbc9152a_21pa1a54b8RaghavendraVattikuti (1).pdf with LlamaParse
2025-11-10 00:45:37,028 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-10 00:45:39,982 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:45:44,590 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:45:47,892 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:45:49,860 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:45:52,325 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:45:55,234 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:45:55,246 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 2 successful, 0 failed
2025-11-10 00:45:55,246 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-10 00:45:55,267 - src.document_processing.indexing_pipeline - INFO - Identified 7 important entities
2025-11-10 00:45:55,267 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_3705d8cc-5975-4dea-aa4d-9e3a5967ae27_4: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:45:55,267 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_3705d8cc-5975-4dea-aa4d-9e3a5967ae27_6: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:45:55,267 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_3705d8cc-5975-4dea-aa4d-9e3a5967ae27_7: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:45:55,267 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_3705d8cc-5975-4dea-aa4d-9e3a5967ae27_10: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:45:55,267 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_3705d8cc-5975-4dea-aa4d-9e3a5967ae27_1: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:45:55,267 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_3705d8cc-5975-4dea-aa4d-9e3a5967ae27_14: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:45:55,267 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_a492a86f-aa8d-46ff-9620-3554db7212a2_6: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:45:55,267 - src.document_processing.indexing_pipeline - INFO - Created 0 attribute nodes
2025-11-10 00:45:55,276 - src.graph.graph_manager - INFO - Detected 6 communities
2025-11-10 00:45:57,676 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:45:59,317 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:03,171 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:03,874 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:06,157 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:06,937 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:10,090 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:11,179 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:15,448 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:16,034 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:19,194 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:19,920 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:46:19,928 - src.document_processing.indexing_pipeline - INFO - Created 6 high-level nodes and 6 overview nodes
2025-11-10 00:46:19,929 - src.document_processing.indexing_pipeline - INFO - Document indexing completed in 53.55 seconds
2025-11-10 00:46:19,932 - src.graph.graph_manager - INFO - Graph saved to data/processed/graph.gpickle
2025-11-10 00:46:19,933 - src.api.routes - INFO - Cleaned up temporary file data/raw/7e77f52d-df2e-41df-8c67-1856bbc9152a_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:46:19,933 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:46:19] "POST /api/upload/document HTTP/1.1" 200 -
2025-11-10 00:46:19,946 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:46:19] "GET /api/graph/stats HTTP/1.1" 200 -
2025-11-10 00:46:26,208 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:46:26] "GET /health HTTP/1.1" 200 -
2025-11-10 00:46:26,230 - src.api.routes - INFO - Graph visualizer initialized
2025-11-10 00:46:26,256 - src.graph.graph_manager - INFO - Detected 6 communities
2025-11-10 00:46:26,256 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:46:26] "GET /api/visualization/stats HTTP/1.1" 200 -
2025-11-10 00:46:28,914 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:46:28] "GET /health HTTP/1.1" 200 -
2025-11-10 00:46:29,096 - src.visualization.graph_visualizer - INFO - Graph visualization saved to: data/processed/graph_visualization.html
2025-11-10 00:46:29,096 - src.visualization.graph_visualizer - INFO - Nodes: 90, Edges: 179
2025-11-10 00:46:29,102 - src.graph.graph_manager - INFO - Detected 6 communities
2025-11-10 00:46:29,103 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:46:29] "POST /api/visualization/create HTTP/1.1" 200 -
2025-11-10 00:46:31,401 - src.api.routes - ERROR - Error in serve_visualization: [Errno 2] No such file or directory: '/Users/raghavendra/new/rapidrfpRag/src/api/data/processed/graph_visualization.html'
2025-11-10 00:46:31,401 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:46:31] "[35m[1mGET /api/visualization/serve/graph_visualization.html HTTP/1.1[0m" 500 -
2025-11-10 00:46:31,492 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:46:31] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2025-11-10 00:46:56,699 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:46:56] "GET /health HTTP/1.1" 200 -
2025-11-10 00:47:08,520 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:47:08] "GET /health HTTP/1.1" 200 -
2025-11-10 00:47:08,527 - src.api.routes - ERROR - Failed to initialize advanced search: AdvancedSearchSystem.__init__() missing 3 required positional arguments: 'graph_manager', 'hnsw_service', and 'llm_service'
2025-11-10 00:47:08,528 - src.api.routes - ERROR - Error in advanced_search_endpoint: AdvancedSearchSystem.__init__() missing 3 required positional arguments: 'graph_manager', 'hnsw_service', and 'llm_service'
2025-11-10 00:47:08,530 - src.api.routes - ERROR - Traceback (most recent call last):
  File "/Users/raghavendra/new/rapidrfpRag/src/api/routes.py", line 697, in advanced_search_endpoint
    search_system = _initialize_advanced_search()
  File "/Users/raghavendra/new/rapidrfpRag/src/api/routes.py", line 665, in _initialize_advanced_search
    advanced_search = AdvancedSearchSystem()
TypeError: AdvancedSearchSystem.__init__() missing 3 required positional arguments: 'graph_manager', 'hnsw_service', and 'llm_service'

2025-11-10 00:47:08,531 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:47:08] "[35m[1mPOST /api/search/advanced HTTP/1.1[0m" 500 -
2025-11-10 00:47:34,971 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:47:34] "GET /health HTTP/1.1" 200 -
2025-11-10 00:47:35,041 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:47:35] "GET /health HTTP/1.1" 200 -
2025-11-10 00:47:35,049 - src.api.routes - ERROR - Failed to initialize advanced search: AdvancedSearchSystem.__init__() missing 3 required positional arguments: 'graph_manager', 'hnsw_service', and 'llm_service'
2025-11-10 00:47:35,049 - src.api.routes - ERROR - Error in answer_generation_endpoint: AdvancedSearchSystem.__init__() missing 3 required positional arguments: 'graph_manager', 'hnsw_service', and 'llm_service'
2025-11-10 00:47:35,052 - src.api.routes - ERROR - Traceback (most recent call last):
  File "/Users/raghavendra/new/rapidrfpRag/src/api/routes.py", line 823, in answer_generation_endpoint
    search_system = _initialize_advanced_search()
  File "/Users/raghavendra/new/rapidrfpRag/src/api/routes.py", line 665, in _initialize_advanced_search
    advanced_search = AdvancedSearchSystem()
TypeError: AdvancedSearchSystem.__init__() missing 3 required positional arguments: 'graph_manager', 'hnsw_service', and 'llm_service'

2025-11-10 00:47:35,053 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:47:35] "[35m[1mPOST /api/answer HTTP/1.1[0m" 500 -
2025-11-10 00:47:47,951 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/api/routes.py', reloading
2025-11-10 00:47:48,223 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:47:55,088 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:47:55,089 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:47:55,122 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:47:55,133 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:47:55,163 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:47:55] "GET /health HTTP/1.1" 200 -
2025-11-10 00:47:55,171 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:47:55] "GET /api/graph/important-entities HTTP/1.1" 200 -
2025-11-10 00:47:55,706 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:47:56,079 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/3a635921-de54-4dfe-9cc7-12dbfc58ce51 "HTTP/1.1 200 OK"
2025-11-10 00:47:57,553 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:47:57] "GET /health HTTP/1.1" 200 -
2025-11-10 00:47:57,568 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:47:57] "GET /api/graph/communities HTTP/1.1" 200 -
2025-11-10 00:48:00,362 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:48:00] "GET /health HTTP/1.1" 200 -
2025-11-10 00:48:00,483 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:48:00] "GET /api/graph/nodes/T HTTP/1.1" 200 -
2025-11-10 00:48:04,391 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:48:04] "GET /health HTTP/1.1" 200 -
2025-11-10 00:48:04,658 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/api/routes.py', reloading
2025-11-10 00:48:04,810 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:48:11,619 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:48:11,619 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:48:11,653 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:48:11,671 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:48:11,700 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:48:11] "GET /health HTTP/1.1" 200 -
2025-11-10 00:48:11,714 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:48:11] "GET /api/graph/nodes/S HTTP/1.1" 200 -
2025-11-10 00:48:11,850 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:48:12,657 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/92f663b5-bd9d-4ce9-9b8a-59ecaa0c3f7a "HTTP/1.1 200 OK"
2025-11-10 00:48:24,728 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:48:24,729 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:48:24,769 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-10 00:48:24,769 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-10 00:48:24,770 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:48:25,349 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:48:25,761 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/2174374d-5ca5-439b-adea-f3e8d2b48dd0 "HTTP/1.1 200 OK"
2025-11-10 00:48:31,241 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:48:31,241 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:48:31,265 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:48:31,273 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:48:31,303 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:48:31] "GET /health HTTP/1.1" 200 -
2025-11-10 00:48:31,304 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:48:31] "GET /health HTTP/1.1" 200 -
2025-11-10 00:48:31,456 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:48:32,237 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/423ee12f-7be0-4355-a87c-eb6b35f92826 "HTTP/1.1 200 OK"
2025-11-10 00:48:32,334 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:48:32] "GET /health HTTP/1.1" 200 -
2025-11-10 00:48:39,297 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:48:39] "GET /health HTTP/1.1" 200 -
2025-11-10 00:48:40,644 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:48:40] "GET /health HTTP/1.1" 200 -
2025-11-10 00:48:40,654 - src.api.routes - INFO - File uploaded to data/raw/d6c46601-9a17-4314-8836-9b8281725439_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf
2025-11-10 00:48:40,654 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/d6c46601-9a17-4314-8836-9b8281725439_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf
2025-11-10 00:48:40,654 - src.document_processing.llamaparse_service - INFO - Using LlamaParse for data/raw/d6c46601-9a17-4314-8836-9b8281725439_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf
2025-11-10 00:48:40,654 - src.document_processing.llamaparse_service - INFO - Submitting parsing job for data/raw/d6c46601-9a17-4314-8836-9b8281725439_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf...
2025-11-10 00:48:43,282 - src.document_processing.llamaparse_service - INFO - Job submitted successfully. Job ID: 1b731587-6703-4281-b5c7-975f3328e27a
2025-11-10 00:48:43,291 - src.document_processing.llamaparse_service - INFO - Waiting for job 1b731587-6703-4281-b5c7-975f3328e27a to complete...
2025-11-10 00:48:43,291 - src.document_processing.llamaparse_service - INFO - Max wait time: 300s, Poll interval: 5s
2025-11-10 00:48:44,650 - src.document_processing.llamaparse_service - INFO - Job still PENDING, waiting 5s...
2025-11-10 00:49:09,479 - src.document_processing.llamaparse_service - INFO - Job 1b731587-6703-4281-b5c7-975f3328e27a completed successfully!
2025-11-10 00:49:10,605 - src.document_processing.llamaparse_service - INFO - Retrieved result for job 1b731587-6703-4281-b5c7-975f3328e27a
2025-11-10 00:49:10,606 - src.document_processing.llamaparse_service - INFO - Successfully parsed data/raw/d6c46601-9a17-4314-8836-9b8281725439_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf
2025-11-10 00:49:10,606 - src.document_processing.llamaparse_service - INFO - Found 1 document(s)
2025-11-10 00:49:10,606 - src.document_processing.llamaparse_service - INFO - Converted LlamaParse result: 1 chunks, 159 tokens
2025-11-10 00:49:10,606 - src.document_processing.llamaparse_service - INFO - Successfully loaded data/raw/d6c46601-9a17-4314-8836-9b8281725439_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf with LlamaParse
2025-11-10 00:49:10,606 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-10 00:49:12,484 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:49:13,863 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:49:16,492 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:49:16,502 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 1 successful, 0 failed
2025-11-10 00:49:16,502 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-10 00:49:16,507 - src.document_processing.indexing_pipeline - INFO - Identified 1 important entities
2025-11-10 00:49:16,508 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_078b0db6-37e1-4aa9-8f33-7cf20ce8e422_1: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:49:16,508 - src.document_processing.indexing_pipeline - INFO - Created 0 attribute nodes
2025-11-10 00:49:16,514 - src.graph.graph_manager - INFO - Detected 2 communities
2025-11-10 00:49:18,455 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:49:19,688 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:49:22,254 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:49:23,494 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:49:23,495 - src.document_processing.indexing_pipeline - INFO - Created 2 high-level nodes and 2 overview nodes
2025-11-10 00:49:23,496 - src.document_processing.indexing_pipeline - INFO - Document indexing completed in 42.84 seconds
2025-11-10 00:49:23,499 - src.graph.graph_manager - INFO - Graph saved to data/processed/graph.gpickle
2025-11-10 00:49:23,503 - src.api.routes - INFO - Cleaned up temporary file data/raw/d6c46601-9a17-4314-8836-9b8281725439_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf
2025-11-10 00:49:23,504 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:49:23] "POST /api/upload/document HTTP/1.1" 200 -
2025-11-10 00:49:23,520 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:49:23] "GET /api/graph/stats HTTP/1.1" 200 -
2025-11-10 00:49:27,008 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:49:27] "GET /health HTTP/1.1" 200 -
2025-11-10 00:49:27,025 - src.api.routes - INFO - Graph visualizer initialized
2025-11-10 00:49:27,147 - src.visualization.graph_visualizer - INFO - Graph visualization saved to: data/processed/graph_visualization.html
2025-11-10 00:49:27,148 - src.visualization.graph_visualizer - INFO - Nodes: 16, Edges: 29
2025-11-10 00:49:27,150 - src.graph.graph_manager - INFO - Detected 2 communities
2025-11-10 00:49:27,150 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:49:27] "POST /api/visualization/create HTTP/1.1" 200 -
2025-11-10 00:49:29,536 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:49:29] "GET /api/visualization/serve/graph_visualization.html HTTP/1.1" 200 -
2025-11-10 00:49:29,596 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:49:29] "[33mGET /api/visualization/serve/lib/bindings/utils.js HTTP/1.1[0m" 404 -
2025-11-10 00:49:48,882 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:49:48] "GET /health HTTP/1.1" 200 -
2025-11-10 00:49:49,028 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:49:49] "GET /health HTTP/1.1" 200 -
2025-11-10 00:49:49,036 - src.vector.hnsw_service - INFO - HNSW service initialized: dim=1536, space=cosine
2025-11-10 00:49:49,036 - src.api.routes - INFO - HNSW service initialized
2025-11-10 00:49:49,050 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 00:49:49,357 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 00:49:50,059 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 00:49:51,141 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 00:49:52,125 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 00:49:52,127 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 00:49:52,136 - src.search.personalized_pagerank - INFO - PPR initialized with 16 nodes, modified=True
2025-11-10 00:49:52,137 - src.search.advanced_search - INFO - Built entity lookup with 4 unique entities
2025-11-10 00:49:52,137 - src.search.advanced_search - INFO - Advanced search system initialized
2025-11-10 00:49:52,137 - src.api.routes - INFO - Advanced search system initialized
2025-11-10 00:49:52,137 - src.search.advanced_search - INFO - Advanced search for query: 'whats coitonic'
2025-11-10 00:49:52,358 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:49:53,099 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/11247082-ab9a-44d9-b7ce-aa75be4506dc "HTTP/1.1 200 OK"
2025-11-10 00:49:53,122 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 00:49:54,115 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=11247082-ab9a-44d9-b7ce-aa75be4506dc "HTTP/1.1 200 OK"
2025-11-10 00:50:04,852 - src.vector.hnsw_service - WARNING - HNSW index is empty
2025-11-10 00:50:05,341 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-10 00:50:05,342 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 00:50:05,343 - src.llm.llm_service - ERROR - Error decomposing query: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 00:50:05,343 - src.search.advanced_search - WARNING - No seed nodes for PPR
2025-11-10 00:50:05,343 - src.search.advanced_search - INFO - Search completed: 0 final nodes
2025-11-10 00:50:05,354 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 00:50:05,745 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 00:50:05,985 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 00:50:06,922 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 00:50:07,974 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 00:50:07,976 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 00:50:08,201 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:50:08,960 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/4e1a4132-c0a2-496c-835a-452dcc5e0bec "HTTP/1.1 200 OK"
2025-11-10 00:50:09,674 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:50:09,685 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:50:09] "POST /api/answer HTTP/1.1" 200 -
2025-11-10 00:50:29,551 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:50:29] "GET /health HTTP/1.1" 200 -
2025-11-10 00:50:29,586 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:50:29] "GET /api/graph/nodes/T HTTP/1.1" 200 -
2025-11-10 00:50:47,796 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:50:47] "GET /health HTTP/1.1" 200 -
2025-11-10 00:50:51,931 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:50:51] "GET /health HTTP/1.1" 200 -
2025-11-10 00:50:51,944 - src.search.advanced_search - INFO - Advanced search for query: 'whats COITONIC'
2025-11-10 00:50:52,967 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 00:50:54,002 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=11247082-ab9a-44d9-b7ce-aa75be4506dc "HTTP/1.1 200 OK"
2025-11-10 00:51:00,021 - src.vector.hnsw_service - WARNING - HNSW index is empty
2025-11-10 00:51:00,660 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-10 00:51:00,661 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 00:51:00,661 - src.llm.llm_service - ERROR - Error decomposing query: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 00:51:00,661 - src.search.advanced_search - WARNING - No seed nodes for PPR
2025-11-10 00:51:00,661 - src.search.advanced_search - INFO - Search completed: 0 final nodes
2025-11-10 00:51:00,673 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 00:51:01,062 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 00:51:01,343 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 00:51:02,392 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 00:51:03,440 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 00:51:03,442 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 00:51:03,665 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:51:04,488 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/e89ea646-47a5-40c8-bb5c-cd8fa76effc1 "HTTP/1.1 200 OK"
2025-11-10 00:51:05,237 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:51:05,247 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:51:05] "POST /api/answer HTTP/1.1" 200 -
2025-11-10 00:52:01,217 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:52:01] "GET /api/graph/stats HTTP/1.1" 200 -
2025-11-10 00:52:10,242 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:52:10] "POST /api/search/entities HTTP/1.1" 200 -
2025-11-10 00:52:22,708 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:52:22] "GET /api/graph/node/N_078b0db6-37e1-4aa9-8f33-7cf20ce8e422_3 HTTP/1.1" 200 -
2025-11-10 00:52:33,679 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:52:33] "GET /api/graph/nodes/S HTTP/1.1" 200 -
2025-11-10 00:52:44,774 - src.search.advanced_search - INFO - Advanced search for query: 'What is Coitonic?'
2025-11-10 00:52:45,805 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 00:52:46,788 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=11247082-ab9a-44d9-b7ce-aa75be4506dc "HTTP/1.1 200 OK"
2025-11-10 00:52:52,207 - src.vector.hnsw_service - WARNING - HNSW index is empty
2025-11-10 00:52:53,465 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-10 00:52:53,466 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 00:52:53,467 - src.llm.llm_service - ERROR - Error decomposing query: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 00:52:53,467 - src.search.advanced_search - WARNING - No seed nodes for PPR
2025-11-10 00:52:53,467 - src.search.advanced_search - INFO - Search completed: 0 final nodes
2025-11-10 00:52:53,477 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 00:52:53,792 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 00:52:54,433 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 00:52:55,515 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 00:52:56,537 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 00:52:56,540 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 00:52:56,773 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:52:57,561 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/285cb3e1-f92e-4cf5-be06-96ef1a1d9b3a "HTTP/1.1 200 OK"
2025-11-10 00:52:58,441 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:52:58,443 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:52:58] "POST /api/answer HTTP/1.1" 200 -
2025-11-10 00:53:08,802 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:53:08] "GET /api/hnsw/stats HTTP/1.1" 200 -
2025-11-10 00:53:18,815 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:53:18] "[31m[1mPOST /api/hnsw/rebuild HTTP/1.1[0m" 400 -
2025-11-10 00:56:04,380 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/config/settings.py', reloading
2025-11-10 00:56:04,588 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:56:10,648 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:56:10,648 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:56:10,670 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:56:10,677 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:56:10,874 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:56:11,636 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/0589f7fe-4e3e-4b23-8e6d-cc22164711b0 "HTTP/1.1 200 OK"
2025-11-10 00:57:08,946 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/document_processing/indexing_pipeline.py', reloading
2025-11-10 00:57:09,118 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:57:15,206 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:57:15,206 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:57:15,232 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:57:15,240 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:57:15,429 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:57:16,187 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/a748cdc6-c016-4241-ad59-4b7b4ed5af3a "HTTP/1.1 200 OK"
2025-11-10 00:57:21,493 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/document_processing/indexing_pipeline.py', reloading
2025-11-10 00:57:21,668 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:57:27,741 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:57:27,741 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:57:27,767 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:57:27,775 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:57:27,964 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:57:28,717 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/0bd9af29-c6ec-44a1-9655-19b5db27dd4a "HTTP/1.1 200 OK"
2025-11-10 00:57:46,500 - werkzeug - INFO -  * Detected change in '/Users/raghavendra/new/rapidrfpRag/src/graph/graph_manager.py', reloading
2025-11-10 00:57:46,658 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:57:53,738 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:57:53,738 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:57:53,766 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:57:53,773 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:57:53,962 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:57:54,741 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/b1b4fa95-a6b2-4edd-a1ae-eeacf8e6fb68 "HTTP/1.1 200 OK"
2025-11-10 00:58:27,238 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:58:27,238 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:58:27,273 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-10 00:58:27,273 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-10 00:58:27,274 - werkzeug - INFO -  * Restarting with stat
2025-11-10 00:58:27,462 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:58:28,183 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/2a71be55-dfc8-4232-bccc-0a6db12d1217 "HTTP/1.1 200 OK"
2025-11-10 00:58:33,145 - root - INFO - Starting RapidRFP RAG System
2025-11-10 00:58:33,145 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 00:58:33,184 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 00:58:33,201 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 00:58:33,232 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:58:33] "GET /health HTTP/1.1" 200 -
2025-11-10 00:58:33,233 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:58:33] "GET /health HTTP/1.1" 200 -
2025-11-10 00:58:33,360 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 00:58:34,129 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/148b617a-40e6-4fe3-8a38-040840afe7c0 "HTTP/1.1 200 OK"
2025-11-10 00:58:34,234 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:58:34] "GET /health HTTP/1.1" 200 -
2025-11-10 00:58:40,114 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:58:40] "GET /health HTTP/1.1" 200 -
2025-11-10 00:58:40,122 - src.api.routes - INFO - Graph visualizer initialized
2025-11-10 00:58:40,253 - src.visualization.graph_visualizer - INFO - Graph visualization saved to: data/processed/graph_visualization.html
2025-11-10 00:58:40,254 - src.visualization.graph_visualizer - INFO - Nodes: 0, Edges: 0
2025-11-10 00:58:40,254 - src.visualization.graph_visualizer - ERROR - Failed to get visualization stats: Connectivity is undefined for the null graph.
2025-11-10 00:58:40,255 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:58:40] "POST /api/visualization/create HTTP/1.1" 200 -
2025-11-10 00:58:47,061 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:58:47] "GET /health HTTP/1.1" 200 -
2025-11-10 00:58:48,489 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 00:58:48] "GET /health HTTP/1.1" 200 -
2025-11-10 00:58:48,501 - src.api.routes - INFO - File uploaded to data/raw/5bc8b566-170a-4320-b19b-c0b0a75fc271_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:58:48,501 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/5bc8b566-170a-4320-b19b-c0b0a75fc271_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:58:48,501 - src.document_processing.llamaparse_service - INFO - Using LlamaParse for data/raw/5bc8b566-170a-4320-b19b-c0b0a75fc271_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:58:48,502 - src.document_processing.llamaparse_service - INFO - Submitting parsing job for data/raw/5bc8b566-170a-4320-b19b-c0b0a75fc271_21pa1a54b8RaghavendraVattikuti (1).pdf...
2025-11-10 00:58:51,203 - src.document_processing.llamaparse_service - INFO - Job submitted successfully. Job ID: a7ea0649-92b4-48bd-a9e5-b494a3c1380e
2025-11-10 00:58:51,204 - src.document_processing.llamaparse_service - INFO - Waiting for job a7ea0649-92b4-48bd-a9e5-b494a3c1380e to complete...
2025-11-10 00:58:51,204 - src.document_processing.llamaparse_service - INFO - Max wait time: 300s, Poll interval: 5s
2025-11-10 00:58:52,801 - src.document_processing.llamaparse_service - INFO - Job still PENDING, waiting 5s...
2025-11-10 00:58:58,631 - src.document_processing.llamaparse_service - INFO - Job a7ea0649-92b4-48bd-a9e5-b494a3c1380e completed successfully!
2025-11-10 00:58:59,630 - src.document_processing.llamaparse_service - INFO - Retrieved result for job a7ea0649-92b4-48bd-a9e5-b494a3c1380e
2025-11-10 00:58:59,632 - src.document_processing.llamaparse_service - INFO - Successfully parsed data/raw/5bc8b566-170a-4320-b19b-c0b0a75fc271_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 00:58:59,632 - src.document_processing.llamaparse_service - INFO - Found 1 document(s)
2025-11-10 00:58:59,633 - src.document_processing.llamaparse_service - INFO - Converted LlamaParse result: 2 chunks, 697 tokens
2025-11-10 00:58:59,633 - src.document_processing.llamaparse_service - INFO - Successfully loaded data/raw/5bc8b566-170a-4320-b19b-c0b0a75fc271_21pa1a54b8RaghavendraVattikuti (1).pdf with LlamaParse
2025-11-10 00:58:59,633 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-10 00:59:03,099 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:05,908 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:07,858 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:09,205 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:11,334 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:14,358 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:14,367 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 2 successful, 0 failed
2025-11-10 00:59:14,367 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-10 00:59:14,388 - src.document_processing.indexing_pipeline - INFO - Identified 6 important entities
2025-11-10 00:59:14,388 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_b79e096b-ac48-4842-ac21-acf0ad0a8fdd_5: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:59:14,388 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_b79e096b-ac48-4842-ac21-acf0ad0a8fdd_3: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:59:14,388 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_b79e096b-ac48-4842-ac21-acf0ad0a8fdd_4: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:59:14,388 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_b79e096b-ac48-4842-ac21-acf0ad0a8fdd_11: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:59:14,388 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_b79e096b-ac48-4842-ac21-acf0ad0a8fdd_8: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:59:14,388 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_b79e096b-ac48-4842-ac21-acf0ad0a8fdd_9: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 00:59:14,388 - src.document_processing.indexing_pipeline - INFO - Created 0 attribute nodes
2025-11-10 00:59:14,396 - src.graph.graph_manager - INFO - Detected 7 communities
2025-11-10 00:59:17,755 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:18,472 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:20,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:20,943 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:23,000 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:23,715 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:27,243 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:27,963 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:31,699 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:32,502 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:35,887 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:37,017 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:41,955 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:42,795 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 00:59:42,822 - src.document_processing.indexing_pipeline - INFO - Created 7 high-level nodes and 7 overview nodes
2025-11-10 00:59:42,822 - src.document_processing.indexing_pipeline - INFO - Starting Phase III: Embedding Generation
2025-11-10 00:59:42,822 - src.document_processing.indexing_pipeline - INFO - Generating embeddings for 88 nodes
2025-11-10 00:59:43,975 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 00:59:45,020 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=37e76014-31ac-41b9-bba5-8bd074dc98be "HTTP/1.1 200 OK"
2025-11-10 00:59:53,135 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 1/6
2025-11-10 00:59:54,117 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 00:59:55,129 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=37e76014-31ac-41b9-bba5-8bd074dc98be "HTTP/1.1 200 OK"
2025-11-10 00:59:56,329 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 2/6
2025-11-10 00:59:57,326 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 00:59:58,324 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=37e76014-31ac-41b9-bba5-8bd074dc98be "HTTP/1.1 200 OK"
2025-11-10 01:00:04,270 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 3/6
2025-11-10 01:00:05,291 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:00:06,226 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=37e76014-31ac-41b9-bba5-8bd074dc98be "HTTP/1.1 200 OK"
2025-11-10 01:00:07,435 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 4/6
2025-11-10 01:00:08,375 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:00:09,349 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=37e76014-31ac-41b9-bba5-8bd074dc98be "HTTP/1.1 200 OK"
2025-11-10 01:00:10,534 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 5/6
2025-11-10 01:00:11,546 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:00:12,536 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=37e76014-31ac-41b9-bba5-8bd074dc98be "HTTP/1.1 200 OK"
2025-11-10 01:00:13,534 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 6/6
2025-11-10 01:00:13,535 - src.document_processing.indexing_pipeline - INFO - Phase III completed: 88 embeddings generated
2025-11-10 01:00:13,536 - src.document_processing.indexing_pipeline - INFO - Document indexing completed in 85.03 seconds
2025-11-10 01:00:13,548 - src.graph.graph_manager - INFO - Graph saved to data/processed/graph.gpickle
2025-11-10 01:00:13,551 - src.api.routes - INFO - Cleaned up temporary file data/raw/5bc8b566-170a-4320-b19b-c0b0a75fc271_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 01:00:13,551 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:00:13] "POST /api/upload/document HTTP/1.1" 200 -
2025-11-10 01:00:13,559 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:00:13] "GET /api/graph/stats HTTP/1.1" 200 -
2025-11-10 01:00:18,661 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:00:18] "GET /health HTTP/1.1" 200 -
2025-11-10 01:00:18,862 - src.visualization.graph_visualizer - INFO - Graph visualization saved to: data/processed/graph_visualization.html
2025-11-10 01:00:18,862 - src.visualization.graph_visualizer - INFO - Nodes: 90, Edges: 190
2025-11-10 01:00:18,986 - src.graph.graph_manager - INFO - Detected 7 communities
2025-11-10 01:00:18,986 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:00:18] "POST /api/visualization/create HTTP/1.1" 200 -
2025-11-10 01:00:20,338 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:00:20] "GET /api/visualization/serve/graph_visualization.html HTTP/1.1" 200 -
2025-11-10 01:00:20,397 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:00:20] "[33mGET /api/visualization/serve/lib/bindings/utils.js HTTP/1.1[0m" 404 -
2025-11-10 01:00:46,933 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:00:46] "GET /health HTTP/1.1" 200 -
2025-11-10 01:00:47,028 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:00:47] "GET /health HTTP/1.1" 200 -
2025-11-10 01:00:47,042 - src.vector.hnsw_service - INFO - HNSW service initialized: dim=1536, space=cosine
2025-11-10 01:00:47,042 - src.api.routes - INFO - HNSW service initialized
2025-11-10 01:00:47,052 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 01:00:47,360 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 01:00:47,618 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 01:00:48,650 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 01:00:49,648 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 01:00:49,651 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 01:00:49,666 - src.search.personalized_pagerank - INFO - PPR initialized with 90 nodes, modified=True
2025-11-10 01:00:49,667 - src.search.advanced_search - INFO - Built entity lookup with 33 unique entities
2025-11-10 01:00:49,667 - src.search.advanced_search - INFO - Advanced search system initialized
2025-11-10 01:00:49,667 - src.api.routes - INFO - Advanced search system initialized
2025-11-10 01:00:49,667 - src.search.advanced_search - INFO - Advanced search for query: 'tell about raghavendra'
2025-11-10 01:00:49,880 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:00:50,698 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:00:50,699 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/9873e627-0751-42d5-9889-06ff97637751 "HTTP/1.1 200 OK"
2025-11-10 01:00:51,694 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=9873e627-0751-42d5-9889-06ff97637751 "HTTP/1.1 200 OK"
2025-11-10 01:00:56,890 - src.vector.hnsw_service - WARNING - HNSW index is empty
2025-11-10 01:00:58,177 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-10 01:00:58,178 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:00:58,178 - src.llm.llm_service - ERROR - Error decomposing query: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:00:58,179 - src.search.advanced_search - WARNING - No seed nodes for PPR
2025-11-10 01:00:58,179 - src.search.advanced_search - INFO - Search completed: 0 final nodes
2025-11-10 01:00:58,190 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 01:00:58,483 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 01:00:58,744 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 01:00:59,735 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 01:01:00,768 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 01:01:00,777 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 01:01:01,042 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:01:01,824 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/c6f82c01-2c96-4aa3-98a2-06bf1402a605 "HTTP/1.1 200 OK"
2025-11-10 01:01:03,856 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:01:03,858 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:01:03] "POST /api/answer HTTP/1.1" 200 -
2025-11-10 01:01:27,744 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:01:27] "GET /health HTTP/1.1" 200 -
2025-11-10 01:01:29,439 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:01:29] "GET /health HTTP/1.1" 200 -
2025-11-10 01:01:29,454 - src.search.advanced_search - INFO - Advanced search for query: 'tell about raghavendra from the document i gave
'
2025-11-10 01:01:30,460 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:01:31,720 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=9873e627-0751-42d5-9889-06ff97637751 "HTTP/1.1 200 OK"
2025-11-10 01:01:32,206 - src.vector.hnsw_service - WARNING - HNSW index is empty
2025-11-10 01:01:33,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-10 01:01:33,024 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:01:33,024 - src.llm.llm_service - ERROR - Error decomposing query: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:01:33,025 - src.search.advanced_search - WARNING - No seed nodes for PPR
2025-11-10 01:01:33,025 - src.search.advanced_search - INFO - Search completed: 0 final nodes
2025-11-10 01:01:33,036 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 01:01:33,351 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 01:01:33,599 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 01:01:34,594 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 01:01:35,643 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 01:01:35,645 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 01:01:35,877 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:01:36,691 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/38959e2a-c7ac-4032-9a5e-a707729bcfd3 "HTTP/1.1 200 OK"
2025-11-10 01:01:39,011 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:01:39,029 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:01:39] "POST /api/answer HTTP/1.1" 200 -
2025-11-10 01:05:24,537 - root - INFO - Starting RapidRFP RAG System
2025-11-10 01:05:24,538 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 01:05:24,578 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-10 01:05:24,578 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-10 01:05:24,579 - werkzeug - INFO -  * Restarting with stat
2025-11-10 01:05:24,763 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:05:25,492 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/aa1325bb-ec76-441f-90c1-53169954c325 "HTTP/1.1 200 OK"
2025-11-10 01:05:30,835 - root - INFO - Starting RapidRFP RAG System
2025-11-10 01:05:30,835 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-10 01:05:30,862 - werkzeug - WARNING -  * Debugger is active!
2025-11-10 01:05:30,871 - werkzeug - INFO -  * Debugger PIN: 175-510-216
2025-11-10 01:05:30,905 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:05:30] "GET /health HTTP/1.1" 200 -
2025-11-10 01:05:30,905 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:05:30] "GET /health HTTP/1.1" 200 -
2025-11-10 01:05:31,072 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:05:31,813 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/5562aa96-b2a9-4efb-ad0a-88984e2d6ec4 "HTTP/1.1 200 OK"
2025-11-10 01:05:31,873 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:05:31] "GET /health HTTP/1.1" 200 -
2025-11-10 01:05:41,943 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:05:41] "GET /health HTTP/1.1" 200 -
2025-11-10 01:05:43,163 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:05:43] "GET /health HTTP/1.1" 200 -
2025-11-10 01:05:43,175 - src.api.routes - INFO - File uploaded to data/raw/d2fc6fdd-a47e-4ded-b2f8-b70f7229aa3f_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 01:05:43,175 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/d2fc6fdd-a47e-4ded-b2f8-b70f7229aa3f_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 01:05:43,175 - src.document_processing.llamaparse_service - INFO - Using LlamaParse for data/raw/d2fc6fdd-a47e-4ded-b2f8-b70f7229aa3f_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 01:05:43,175 - src.document_processing.llamaparse_service - INFO - Submitting parsing job for data/raw/d2fc6fdd-a47e-4ded-b2f8-b70f7229aa3f_21pa1a54b8RaghavendraVattikuti (1).pdf...
2025-11-10 01:05:46,142 - src.document_processing.llamaparse_service - INFO - Job submitted successfully. Job ID: 71a6e096-6d49-4306-82f2-3ab414ba0afb
2025-11-10 01:05:46,144 - src.document_processing.llamaparse_service - INFO - Waiting for job 71a6e096-6d49-4306-82f2-3ab414ba0afb to complete...
2025-11-10 01:05:46,144 - src.document_processing.llamaparse_service - INFO - Max wait time: 300s, Poll interval: 5s
2025-11-10 01:05:46,969 - src.document_processing.llamaparse_service - INFO - Job still PENDING, waiting 5s...
2025-11-10 01:05:52,807 - src.document_processing.llamaparse_service - INFO - Job 71a6e096-6d49-4306-82f2-3ab414ba0afb completed successfully!
2025-11-10 01:05:53,799 - src.document_processing.llamaparse_service - INFO - Retrieved result for job 71a6e096-6d49-4306-82f2-3ab414ba0afb
2025-11-10 01:05:53,801 - src.document_processing.llamaparse_service - INFO - Successfully parsed data/raw/d2fc6fdd-a47e-4ded-b2f8-b70f7229aa3f_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 01:05:53,801 - src.document_processing.llamaparse_service - INFO - Found 1 document(s)
2025-11-10 01:05:53,801 - src.document_processing.llamaparse_service - INFO - Converted LlamaParse result: 2 chunks, 697 tokens
2025-11-10 01:05:53,801 - src.document_processing.llamaparse_service - INFO - Successfully loaded data/raw/d2fc6fdd-a47e-4ded-b2f8-b70f7229aa3f_21pa1a54b8RaghavendraVattikuti (1).pdf with LlamaParse
2025-11-10 01:05:53,801 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-10 01:05:56,706 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:05:59,230 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:01,364 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:03,200 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:06,940 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:09,233 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:09,245 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 2 successful, 0 failed
2025-11-10 01:06:09,245 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-10 01:06:09,261 - src.document_processing.indexing_pipeline - INFO - Identified 7 important entities
2025-11-10 01:06:09,262 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_5: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:06:09,262 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_7: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:06:09,262 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_8: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:06:09,262 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_11: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:06:09,262 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_1: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:06:09,262 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_15: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:06:09,262 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_0a8609aa-cedb-4b8e-9470-8222afc90a30_6: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:06:09,262 - src.document_processing.indexing_pipeline - INFO - Created 0 attribute nodes
2025-11-10 01:06:09,267 - src.graph.graph_manager - INFO - Detected 5 communities
2025-11-10 01:06:11,669 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:12,349 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:15,382 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:16,460 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:19,354 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:20,048 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:21,750 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:22,315 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:24,952 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:25,684 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:06:25,701 - src.document_processing.indexing_pipeline - INFO - Created 5 high-level nodes and 5 overview nodes
2025-11-10 01:06:25,701 - src.document_processing.indexing_pipeline - INFO - Starting Phase III: Embedding Generation
2025-11-10 01:06:25,702 - src.document_processing.indexing_pipeline - INFO - Generating embeddings for 78 nodes
2025-11-10 01:06:26,742 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:06:27,742 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:06:34,704 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 1/5
2025-11-10 01:06:36,413 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:06:37,379 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:06:38,559 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 2/5
2025-11-10 01:06:39,518 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:06:40,506 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:06:41,718 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 3/5
2025-11-10 01:06:42,795 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:06:43,815 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:06:45,076 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 4/5
2025-11-10 01:06:46,085 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:06:47,091 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:06:48,333 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 5/5
2025-11-10 01:06:48,333 - src.document_processing.indexing_pipeline - INFO - Phase III completed: 78 embeddings generated
2025-11-10 01:06:48,333 - src.document_processing.indexing_pipeline - INFO - Document indexing completed in 65.16 seconds
2025-11-10 01:06:48,342 - src.graph.graph_manager - INFO - Graph saved to data/processed/graph.gpickle
2025-11-10 01:06:48,343 - src.api.routes - INFO - Cleaned up temporary file data/raw/d2fc6fdd-a47e-4ded-b2f8-b70f7229aa3f_21pa1a54b8RaghavendraVattikuti (1).pdf
2025-11-10 01:06:48,344 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:06:48] "POST /api/upload/document HTTP/1.1" 200 -
2025-11-10 01:06:48,363 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:06:48] "GET /api/graph/stats HTTP/1.1" 200 -
2025-11-10 01:06:58,239 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:06:58] "GET /health HTTP/1.1" 200 -
2025-11-10 01:06:58,346 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:06:58] "GET /health HTTP/1.1" 200 -
2025-11-10 01:06:58,356 - src.vector.hnsw_service - INFO - HNSW service initialized: dim=1536, space=cosine
2025-11-10 01:06:58,356 - src.api.routes - INFO - HNSW service initialized
2025-11-10 01:06:58,365 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 01:06:58,688 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 01:06:59,272 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 01:07:00,263 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 01:07:01,257 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 01:07:01,259 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 01:07:01,287 - src.search.personalized_pagerank - INFO - PPR initialized with 80 nodes, modified=True
2025-11-10 01:07:01,288 - src.search.advanced_search - INFO - Built entity lookup with 36 unique entities
2025-11-10 01:07:01,288 - src.search.advanced_search - INFO - Advanced search system initialized
2025-11-10 01:07:01,288 - src.api.routes - INFO - Advanced search system initialized
2025-11-10 01:07:01,288 - src.search.advanced_search - INFO - Advanced search for query: 'tell raghavendra skills '
2025-11-10 01:07:01,492 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:07:02,267 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/b8c7ba6c-751d-40b9-8897-079aed37619c "HTTP/1.1 200 OK"
2025-11-10 01:07:02,273 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:07:03,272 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=b8c7ba6c-751d-40b9-8897-079aed37619c "HTTP/1.1 200 OK"
2025-11-10 01:07:03,761 - src.vector.hnsw_service - WARNING - HNSW index is empty
2025-11-10 01:07:04,572 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-10 01:07:04,572 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:07:04,572 - src.llm.llm_service - ERROR - Error decomposing query: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:07:04,572 - src.search.advanced_search - WARNING - No seed nodes for PPR
2025-11-10 01:07:04,572 - src.search.advanced_search - INFO - Search completed: 0 final nodes
2025-11-10 01:07:04,577 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 01:07:04,839 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 01:07:05,084 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 01:07:07,072 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 01:07:08,082 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 01:07:08,084 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 01:07:08,316 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:07:09,093 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/2c5a56c0-38ec-4215-bed3-804f6dc17e69 "HTTP/1.1 200 OK"
2025-11-10 01:07:10,207 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:07:10,221 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:07:10] "POST /api/answer HTTP/1.1" 200 -
2025-11-10 01:07:18,653 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:07:18] "GET /health HTTP/1.1" 200 -
2025-11-10 01:07:18,672 - src.api.routes - INFO - Graph visualizer initialized
2025-11-10 01:07:18,815 - src.graph.graph_manager - INFO - Detected 5 communities
2025-11-10 01:07:18,816 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:07:18] "GET /api/visualization/stats HTTP/1.1" 200 -
2025-11-10 01:07:22,996 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:07:22] "GET /health HTTP/1.1" 200 -
2025-11-10 01:07:23,262 - src.visualization.graph_visualizer - INFO - Graph visualization saved to: data/processed/graph_visualization.html
2025-11-10 01:07:23,263 - src.visualization.graph_visualizer - INFO - Nodes: 80, Edges: 154
2025-11-10 01:07:23,378 - src.graph.graph_manager - INFO - Detected 5 communities
2025-11-10 01:07:23,378 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:07:23] "POST /api/visualization/create HTTP/1.1" 200 -
2025-11-10 01:07:24,676 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:07:24] "GET /api/visualization/serve/graph_visualization.html HTTP/1.1" 200 -
2025-11-10 01:07:24,737 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:07:24] "[33mGET /api/visualization/serve/lib/bindings/utils.js HTTP/1.1[0m" 404 -
2025-11-10 01:07:49,700 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:07:49] "GET /health HTTP/1.1" 200 -
2025-11-10 01:07:49,724 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:07:49] "GET /api/graph/nodes/T HTTP/1.1" 200 -
2025-11-10 01:08:48,922 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:08:48] "GET /health HTTP/1.1" 200 -
2025-11-10 01:08:49,142 - src.visualization.graph_visualizer - INFO - Graph visualization saved to: data/processed/graph_visualization.html
2025-11-10 01:08:49,142 - src.visualization.graph_visualizer - INFO - Nodes: 80, Edges: 154
2025-11-10 01:08:49,253 - src.graph.graph_manager - INFO - Detected 5 communities
2025-11-10 01:08:49,253 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:08:49] "POST /api/visualization/create HTTP/1.1" 200 -
2025-11-10 01:08:50,196 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:08:50] "GET /api/visualization/serve/graph_visualization.html HTTP/1.1" 200 -
2025-11-10 01:08:50,242 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:08:50] "[33mGET /api/visualization/serve/lib/bindings/utils.js HTTP/1.1[0m" 404 -
2025-11-10 01:09:32,883 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:09:32] "GET /health HTTP/1.1" 200 -
2025-11-10 01:09:33,905 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:09:33] "GET /health HTTP/1.1" 200 -
2025-11-10 01:09:33,911 - src.api.routes - INFO - File uploaded to data/raw/51a25f4f-1f2c-45b7-9e92-7dd4e0b09a6f_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf
2025-11-10 01:09:33,911 - src.document_processing.indexing_pipeline - INFO - Starting indexing pipeline for data/raw/51a25f4f-1f2c-45b7-9e92-7dd4e0b09a6f_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf
2025-11-10 01:09:33,912 - src.document_processing.llamaparse_service - INFO - Using LlamaParse for data/raw/51a25f4f-1f2c-45b7-9e92-7dd4e0b09a6f_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf
2025-11-10 01:09:33,912 - src.document_processing.llamaparse_service - INFO - Submitting parsing job for data/raw/51a25f4f-1f2c-45b7-9e92-7dd4e0b09a6f_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf...
2025-11-10 01:09:36,620 - src.document_processing.llamaparse_service - INFO - Job submitted successfully. Job ID: c0dc927f-57bc-48e9-bff3-4b80160a9c3b
2025-11-10 01:09:36,622 - src.document_processing.llamaparse_service - INFO - Waiting for job c0dc927f-57bc-48e9-bff3-4b80160a9c3b to complete...
2025-11-10 01:09:36,623 - src.document_processing.llamaparse_service - INFO - Max wait time: 300s, Poll interval: 5s
2025-11-10 01:09:37,468 - src.document_processing.llamaparse_service - INFO - Job still PENDING, waiting 5s...
2025-11-10 01:09:43,315 - src.document_processing.llamaparse_service - INFO - Job c0dc927f-57bc-48e9-bff3-4b80160a9c3b completed successfully!
2025-11-10 01:09:44,283 - src.document_processing.llamaparse_service - INFO - Retrieved result for job c0dc927f-57bc-48e9-bff3-4b80160a9c3b
2025-11-10 01:09:44,285 - src.document_processing.llamaparse_service - INFO - Successfully parsed data/raw/51a25f4f-1f2c-45b7-9e92-7dd4e0b09a6f_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf
2025-11-10 01:09:44,285 - src.document_processing.llamaparse_service - INFO - Found 1 document(s)
2025-11-10 01:09:44,286 - src.document_processing.llamaparse_service - INFO - Converted LlamaParse result: 1 chunks, 159 tokens
2025-11-10 01:09:44,286 - src.document_processing.llamaparse_service - INFO - Successfully loaded data/raw/51a25f4f-1f2c-45b7-9e92-7dd4e0b09a6f_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf with LlamaParse
2025-11-10 01:09:44,286 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-10 01:09:45,843 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:09:47,650 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:09:50,828 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:09:50,831 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 1 successful, 0 failed
2025-11-10 01:09:50,831 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-10 01:09:50,920 - src.document_processing.indexing_pipeline - INFO - Identified 8 important entities
2025-11-10 01:09:50,920 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_5: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:09:50,920 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_7: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:09:50,920 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_8: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:09:50,920 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_11: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:09:50,920 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_15: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:09:50,921 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_0a8609aa-cedb-4b8e-9470-8222afc90a30_6: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:09:50,921 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_52e5cd32-ebf2-406a-a532-8aa6f030f5b7_0: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:09:50,921 - src.document_processing.indexing_pipeline - ERROR - Error creating attribute node for entity N_0a8609aa-cedb-4b8e-9470-8222afc90a30_7: LLMService.generate_entity_attributes() missing 1 required positional argument: 'relationships'
2025-11-10 01:09:50,921 - src.document_processing.indexing_pipeline - INFO - Created 0 attribute nodes
2025-11-10 01:09:50,950 - src.graph.graph_manager - INFO - Detected 6 communities
2025-11-10 01:09:55,419 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:09:57,203 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:09:59,989 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:10:00,513 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:10:02,668 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:10:03,802 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:10:05,665 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:10:06,431 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:10:08,175 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:10:11,995 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:10:14,973 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:10:15,611 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:10:15,616 - src.document_processing.indexing_pipeline - INFO - Created 6 high-level nodes and 6 overview nodes
2025-11-10 01:10:15,617 - src.document_processing.indexing_pipeline - INFO - Starting Phase III: Embedding Generation
2025-11-10 01:10:15,618 - src.document_processing.indexing_pipeline - INFO - Generating embeddings for 93 nodes
2025-11-10 01:10:16,653 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:10:17,625 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:10:23,500 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 1/6
2025-11-10 01:10:24,467 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:10:25,432 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:10:25,483 - src.llm.llm_service - ERROR - Error getting embeddings: You have exceeded your GPU quota (60s requested vs. 58s left). Try again in 23:39:28
2025-11-10 01:10:25,483 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 2/6
2025-11-10 01:10:26,486 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:10:27,504 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:10:27,506 - src.llm.llm_service - ERROR - Error getting embeddings: You have exceeded your GPU quota (60s requested vs. 58s left). Try again in 23:39:26
2025-11-10 01:10:27,506 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 3/6
2025-11-10 01:10:28,482 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:10:29,438 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:10:29,443 - src.llm.llm_service - ERROR - Error getting embeddings: You have exceeded your GPU quota (60s requested vs. 58s left). Try again in 23:39:24
2025-11-10 01:10:29,444 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 4/6
2025-11-10 01:10:30,376 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:10:31,337 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:10:31,344 - src.llm.llm_service - ERROR - Error getting embeddings: You have exceeded your GPU quota (60s requested vs. 58s left). Try again in 23:39:22
2025-11-10 01:10:31,344 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 5/6
2025-11-10 01:10:32,356 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:10:33,323 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=99b2eb52-558a-4d93-8c3b-6d5e82ef52a2 "HTTP/1.1 200 OK"
2025-11-10 01:10:33,372 - src.llm.llm_service - ERROR - Error getting embeddings: You have exceeded your GPU quota (60s requested vs. 58s left). Try again in 23:39:20
2025-11-10 01:10:33,373 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 6/6
2025-11-10 01:10:33,373 - src.document_processing.indexing_pipeline - INFO - Phase III completed: 93 embeddings generated
2025-11-10 01:10:33,374 - src.document_processing.indexing_pipeline - INFO - Document indexing completed in 59.46 seconds
2025-11-10 01:10:33,389 - src.graph.graph_manager - INFO - Graph saved to data/processed/graph.gpickle
2025-11-10 01:10:33,389 - src.api.routes - INFO - Cleaned up temporary file data/raw/51a25f4f-1f2c-45b7-9e92-7dd4e0b09a6f_COTTONIC AFFILIATE SPONSORSHIP AGREEMENT (4).pdf
2025-11-10 01:10:33,390 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:10:33] "POST /api/upload/document HTTP/1.1" 200 -
2025-11-10 01:10:33,402 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:10:33] "GET /api/graph/stats HTTP/1.1" 200 -
2025-11-10 01:11:33,097 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:11:33] "GET /health HTTP/1.1" 200 -
2025-11-10 01:11:33,187 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:11:33] "GET /health HTTP/1.1" 200 -
2025-11-10 01:11:33,194 - src.search.advanced_search - INFO - Advanced search for query: 'tell the commision percentage only of the sponsorpship
'
2025-11-10 01:11:34,530 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:11:35,511 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=b8c7ba6c-751d-40b9-8897-079aed37619c "HTTP/1.1 200 OK"
2025-11-10 01:11:35,549 - src.llm.llm_service - ERROR - Error getting embeddings: You have exceeded your GPU quota (60s requested vs. 58s left). Try again in 23:38:18
2025-11-10 01:11:35,549 - src.vector.hnsw_service - WARNING - HNSW index is empty
2025-11-10 01:11:36,811 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-10 01:11:36,813 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:11:36,813 - src.llm.llm_service - ERROR - Error decomposing query: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:11:36,813 - src.search.advanced_search - WARNING - No seed nodes for PPR
2025-11-10 01:11:36,813 - src.search.advanced_search - INFO - Search completed: 0 final nodes
2025-11-10 01:11:36,827 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 01:11:38,198 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 01:11:38,441 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 01:11:39,429 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 01:11:40,421 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 01:11:40,422 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 01:11:40,649 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:11:41,379 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/86ecc41a-5321-4470-ba36-d18a2826532c "HTTP/1.1 200 OK"
2025-11-10 01:11:42,805 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:11:42,821 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:11:42] "POST /api/answer HTTP/1.1" 200 -
2025-11-10 01:12:31,274 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:12:31] "GET /health HTTP/1.1" 200 -
2025-11-10 01:12:31,348 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:12:31] "GET /health HTTP/1.1" 200 -
2025-11-10 01:12:31,356 - src.search.advanced_search - INFO - Advanced search for query: 'tell the code rule mentioned in the doc'
2025-11-10 01:12:32,491 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:12:33,500 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=b8c7ba6c-751d-40b9-8897-079aed37619c "HTTP/1.1 200 OK"
2025-11-10 01:12:33,505 - src.llm.llm_service - ERROR - Error getting embeddings: You have exceeded your GPU quota (60s requested vs. 58s left). Try again in 23:37:20
2025-11-10 01:12:33,506 - src.vector.hnsw_service - WARNING - HNSW index is empty
2025-11-10 01:12:34,015 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-10 01:12:34,015 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:12:34,015 - src.llm.llm_service - ERROR - Error decomposing query: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:12:34,016 - src.search.advanced_search - WARNING - No seed nodes for PPR
2025-11-10 01:12:34,016 - src.search.advanced_search - INFO - Search completed: 0 final nodes
2025-11-10 01:12:34,025 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 01:12:34,339 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 01:12:34,969 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 01:12:35,995 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 01:12:37,043 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 01:12:37,045 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 01:12:37,269 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:12:38,111 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/30506728-7f38-4f04-a349-ac937032a667 "HTTP/1.1 200 OK"
2025-11-10 01:12:38,909 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:12:38,940 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:12:38] "POST /api/answer HTTP/1.1" 200 -
2025-11-10 01:13:12,678 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:13:12] "GET /health HTTP/1.1" 200 -
2025-11-10 01:13:12,840 - src.visualization.graph_visualizer - INFO - Graph visualization saved to: data/processed/graph_visualization.html
2025-11-10 01:13:12,840 - src.visualization.graph_visualizer - INFO - Nodes: 96, Edges: 223
2025-11-10 01:13:12,896 - src.graph.graph_manager - INFO - Detected 4 communities
2025-11-10 01:13:12,896 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:13:12] "POST /api/visualization/create HTTP/1.1" 200 -
2025-11-10 01:13:14,070 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:13:14] "GET /api/visualization/serve/graph_visualization.html HTTP/1.1" 200 -
2025-11-10 01:13:14,129 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:13:14] "[33mGET /api/visualization/serve/lib/bindings/utils.js HTTP/1.1[0m" 404 -
2025-11-10 01:13:25,172 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:13:25] "GET /health HTTP/1.1" 200 -
2025-11-10 01:13:25,249 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:13:25] "GET /health HTTP/1.1" 200 -
2025-11-10 01:13:25,255 - src.search.advanced_search - INFO - Advanced search for query: 'tell about raghavendra
'
2025-11-10 01:13:26,330 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:13:27,321 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=b8c7ba6c-751d-40b9-8897-079aed37619c "HTTP/1.1 200 OK"
2025-11-10 01:13:27,346 - src.llm.llm_service - ERROR - Error getting embeddings: You have exceeded your GPU quota (60s requested vs. 58s left). Try again in 23:36:26
2025-11-10 01:13:27,346 - src.vector.hnsw_service - WARNING - HNSW index is empty
2025-11-10 01:13:28,015 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-10 01:13:28,016 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:13:28,016 - src.llm.llm_service - ERROR - Error decomposing query: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:13:28,017 - src.search.advanced_search - WARNING - No seed nodes for PPR
2025-11-10 01:13:28,017 - src.search.advanced_search - INFO - Search completed: 0 final nodes
2025-11-10 01:13:28,028 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 01:13:28,350 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 01:13:28,599 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 01:13:29,576 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 01:13:30,642 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 01:13:30,645 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 01:13:30,872 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:13:31,731 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/edefac2f-7001-4192-b9e5-3e00c9884e5f "HTTP/1.1 200 OK"
2025-11-10 01:13:34,537 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:13:34,540 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:13:34] "POST /api/answer HTTP/1.1" 200 -
2025-11-10 01:13:51,088 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:13:51] "GET /health HTTP/1.1" 200 -
2025-11-10 01:13:55,083 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:13:55] "GET /health HTTP/1.1" 200 -
2025-11-10 01:14:00,486 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:14:00] "GET /health HTTP/1.1" 200 -
2025-11-10 01:14:00,583 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:14:00] "GET /health HTTP/1.1" 200 -
2025-11-10 01:14:00,589 - src.search.advanced_search - INFO - Advanced search for query: 'what is the example given in the document
'
2025-11-10 01:14:01,637 - httpx - INFO - HTTP Request: POST https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/join "HTTP/1.1 200 OK"
2025-11-10 01:14:02,633 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/queue/data?session_hash=b8c7ba6c-751d-40b9-8897-079aed37619c "HTTP/1.1 200 OK"
2025-11-10 01:14:02,665 - src.llm.llm_service - ERROR - Error getting embeddings: You have exceeded your GPU quota (60s requested vs. 58s left). Try again in 23:35:51
2025-11-10 01:14:02,665 - src.vector.hnsw_service - WARNING - HNSW index is empty
2025-11-10 01:14:03,130 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-10 01:14:03,131 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:14:03,132 - src.llm.llm_service - ERROR - Error decomposing query: Error code: 400 - {'error': {'message': "'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}
2025-11-10 01:14:03,132 - src.search.advanced_search - WARNING - No seed nodes for PPR
2025-11-10 01:14:03,132 - src.search.advanced_search - INFO - Search completed: 0 final nodes
2025-11-10 01:14:03,143 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-10 01:14:03,447 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-10 01:14:03,686 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-10 01:14:04,672 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-10 01:14:05,645 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-10 01:14:05,647 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-10 01:14:05,884 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-10 01:14:06,600 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/0c30be3a-d53e-489b-8032-227baa1f86fa "HTTP/1.1 200 OK"
2025-11-10 01:14:06,865 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-10 01:14:06,873 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:14:06] "POST /api/answer HTTP/1.1" 200 -
2025-11-10 01:15:39,094 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:15:39] "GET /api/graph/stats HTTP/1.1" 200 -
2025-11-10 01:15:45,336 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:15:45] "GET /api/hnsw/stats HTTP/1.1" 200 -
2025-11-10 01:16:43,245 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:16:43] "GET /api/graph/stats HTTP/1.1" 200 -
2025-11-10 01:16:57,334 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:16:57] "POST /api/search/entities HTTP/1.1" 200 -
2025-11-10 01:17:09,560 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:17:09] "POST /api/search/entities HTTP/1.1" 200 -
2025-11-10 01:17:16,479 - werkzeug - INFO - 127.0.0.1 - - [10/Nov/2025 01:17:16] "GET /api/hnsw/stats HTTP/1.1" 200 -
2025-11-15 22:00:11,184 - root - INFO - Starting RapidRFP RAG System
2025-11-15 22:00:11,184 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-15 22:00:11,278 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-15 22:00:11,278 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-15 22:00:11,278 - werkzeug - INFO -  * Restarting with stat
2025-11-15 22:00:11,419 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-15 22:00:12,440 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/1c226894-d8f1-47e3-856c-09372ee64ada "HTTP/1.1 200 OK"
2025-11-15 22:00:18,515 - root - INFO - Starting RapidRFP RAG System
2025-11-15 22:00:18,516 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-15 22:00:18,540 - werkzeug - WARNING -  * Debugger is active!
2025-11-15 22:00:18,549 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-15 22:00:18,751 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-15 22:00:19,608 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/76775138-818c-4ad5-a920-e51dd95171c5 "HTTP/1.1 200 OK"
2025-11-15 22:01:15,230 - root - INFO - Starting RapidRFP RAG System
2025-11-15 22:01:15,231 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-15 22:01:15,265 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-15 22:01:15,265 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-15 22:01:15,266 - werkzeug - INFO -  * Restarting with stat
2025-11-15 22:01:15,465 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-15 22:01:16,298 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/65bf7479-18d8-4af7-bf3e-b8a0fd2fa72f "HTTP/1.1 200 OK"
2025-11-15 22:01:21,817 - root - INFO - Starting RapidRFP RAG System
2025-11-15 22:01:21,817 - root - INFO - Starting Flask API server on http://0.0.0.0:5001
2025-11-15 22:01:21,843 - werkzeug - WARNING -  * Debugger is active!
2025-11-15 22:01:21,852 - werkzeug - INFO -  * Debugger PIN: 548-907-297
2025-11-15 22:01:21,885 - werkzeug - INFO - 127.0.0.1 - - [15/Nov/2025 22:01:21] "GET /health HTTP/1.1" 200 -
2025-11-15 22:01:21,885 - werkzeug - INFO - 127.0.0.1 - - [15/Nov/2025 22:01:21] "GET /health HTTP/1.1" 200 -
2025-11-15 22:01:22,047 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-15 22:01:22,994 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/c8013f52-29ad-40e0-9917-b5cc84891a03 "HTTP/1.1 200 OK"
2025-11-15 22:01:23,054 - werkzeug - INFO - 127.0.0.1 - - [15/Nov/2025 22:01:23] "GET /health HTTP/1.1" 200 -
2025-11-26 00:55:35,696 - __main__ - INFO - ✅ Environment check passed
2025-11-26 00:55:35,697 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 00:55:35,713 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 00:55:35,713 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:02:59,207 - api_service - INFO - 🚀 Starting NodeRAG processing: file_id=1e6805e5-b524-42af-a60e-e91ecd4874ed, org_id=12aff77d-e387-4f4b-93bd-b294756dd96f, chunks=5
2025-11-26 01:02:59,234 - api_service - INFO - ✅ Webhook sent successfully: processing
2025-11-26 01:02:59,240 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:02:59] "[35m[1mPOST /api/v1/process-document HTTP/1.1[0m" 202 -
2025-11-26 01:02:59,270 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:02:59,634 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:02:59,906 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:03:01,173 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:03:02,248 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:03:02,251 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:03:02,252 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 01:03:02,252 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 01:03:02,488 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:03:03,262 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/839d1cab-6ca4-4bd3-905a-7998c0f02c2a "HTTP/1.1 200 OK"
2025-11-26 01:03:05,622 - api_service - INFO - ✅ Webhook sent successfully: phase1_started
2025-11-26 01:03:05,622 - api_service - INFO - 🔄 Phase 1: Graph Decomposition
2025-11-26 01:03:05,622 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-26 01:03:08,819 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:11,084 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:14,200 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:16,614 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:18,401 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:20,972 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:21,910 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:23,032 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:25,438 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:26,891 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:29,590 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:32,559 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:33,121 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:35,950 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:35,954 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 5 successful, 0 failed
2025-11-26 01:03:35,978 - api_service - INFO - ✅ Webhook sent successfully: phase2_started
2025-11-26 01:03:35,979 - api_service - INFO - 🔄 Phase 2: Graph Augmentation
2025-11-26 01:03:35,979 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-26 01:03:36,001 - src.document_processing.indexing_pipeline - INFO - Identified 12 important entities
2025-11-26 01:03:37,987 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:39,253 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:41,014 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:42,959 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:44,126 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:45,398 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:49,124 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:50,480 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:51,898 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:53,966 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:56,351 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:58,687 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:03:58,689 - src.document_processing.indexing_pipeline - INFO - Created 12 attribute nodes
2025-11-26 01:04:00,035 - src.graph.graph_manager - INFO - Detected 5 communities
2025-11-26 01:04:02,087 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:04:04,316 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:04:07,034 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:04:08,092 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:04:11,884 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:04:12,476 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:04:18,063 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:04:18,742 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:04:22,603 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:04:23,156 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:04:23,157 - src.document_processing.indexing_pipeline - INFO - Created 5 high-level nodes and 5 overview nodes
2025-11-26 01:04:23,170 - api_service - INFO - ✅ Webhook sent successfully: phase3_started
2025-11-26 01:04:23,170 - api_service - INFO - 🔄 Phase 3: Embedding Generation
2025-11-26 01:04:23,170 - src.document_processing.indexing_pipeline - INFO - Starting Phase III: Embedding Generation
2025-11-26 01:04:23,172 - src.vector.hnsw_service - INFO - HNSW service initialized: dim=1536, space=cosine
2025-11-26 01:04:23,173 - src.vector.hnsw_service - ERROR - HNSW index files not found: data/processed/hnsw_index
2025-11-26 01:04:23,173 - src.document_processing.indexing_pipeline - INFO - Created new empty HNSW index
2025-11-26 01:04:23,173 - src.document_processing.indexing_pipeline - INFO - HNSW service initialized: True
2025-11-26 01:04:23,174 - src.document_processing.indexing_pipeline - INFO - Generating embeddings for 170 nodes
2025-11-26 01:04:24,121 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:25,145 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 1/11
2025-11-26 01:04:25,933 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:26,959 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 2/11
2025-11-26 01:04:28,921 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:30,120 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 3/11
2025-11-26 01:04:31,123 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:32,383 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 4/11
2025-11-26 01:04:34,039 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:35,095 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 5/11
2025-11-26 01:04:35,945 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:36,858 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 6/11
2025-11-26 01:04:37,429 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:37,738 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 7/11
2025-11-26 01:04:39,149 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:40,408 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 8/11
2025-11-26 01:04:42,044 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:43,153 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 9/11
2025-11-26 01:04:43,917 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:44,933 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 10/11
2025-11-26 01:04:45,795 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:46,566 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 11/11
2025-11-26 01:04:46,570 - src.vector.hnsw_service - INFO - HNSW index saved to data/processed/hnsw_index
2025-11-26 01:04:46,570 - src.document_processing.indexing_pipeline - INFO - HNSW index saved: True
2025-11-26 01:04:46,570 - src.document_processing.indexing_pipeline - INFO - Phase III completed: 170 embeddings generated and indexed
2025-11-26 01:04:46,570 - api_service - INFO - 💾 Storing in NeonDB
2025-11-26 01:04:48,476 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:04:48,857 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:04:49,110 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:04:50,181 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:04:51,305 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:04:51,309 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:04:51,542 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:04:52,331 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/b801cc6f-022d-45e1-93ff-89a4eadc4c42 "HTTP/1.1 200 OK"
2025-11-26 01:04:54,377 - src.storage.neon_storage - INFO - ✅ NodeRAG database tables ensured
2025-11-26 01:04:57,483 - src.storage.neon_storage - ERROR - ❌ Error storing NodeRAG data: column "updated_at" of relation "noderag_graphs" does not exist
2025-11-26 01:04:57,486 - api_service - ERROR - ❌ Processing pipeline error: Storage failed: column "updated_at" of relation "noderag_graphs" does not exist
2025-11-26 01:04:57,506 - api_service - INFO - ✅ Webhook sent successfully: failed
2025-11-26 01:07:04,032 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:07:04,032 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:07:04,038 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:07:04,038 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:12:02,585 - api_service - INFO - 🚀 Starting NodeRAG processing: file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, org_id=12aff77d-e387-4f4b-93bd-b294756dd96f, chunks=1
2025-11-26 01:12:02,597 - api_service - INFO - ✅ Webhook sent successfully: processing
2025-11-26 01:12:02,601 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:12:02] "[35m[1mPOST /api/v1/process-document HTTP/1.1[0m" 202 -
2025-11-26 01:12:02,628 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:12:03,389 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:12:04,068 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:12:05,094 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:12:06,162 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:12:06,165 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:12:06,165 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 01:12:06,166 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 01:12:06,261 - api_service - INFO - ✅ Webhook sent successfully: phase1_started
2025-11-26 01:12:06,261 - api_service - INFO - 🔄 Phase 1: Graph Decomposition
2025-11-26 01:12:06,261 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-26 01:12:06,418 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:12:07,186 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/c4a962b0-b91f-466e-a774-ca0a908787cd "HTTP/1.1 200 OK"
2025-11-26 01:12:07,357 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:12:09,928 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:12:12,465 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:12:12,469 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 1 successful, 0 failed
2025-11-26 01:12:12,486 - api_service - INFO - ✅ Webhook sent successfully: phase2_started
2025-11-26 01:12:12,486 - api_service - INFO - 🔄 Phase 2: Graph Augmentation
2025-11-26 01:12:12,487 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-26 01:12:12,490 - src.document_processing.indexing_pipeline - INFO - Identified 1 important entities
2025-11-26 01:12:14,276 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:12:14,278 - src.document_processing.indexing_pipeline - INFO - Created 1 attribute nodes
2025-11-26 01:12:14,321 - src.graph.graph_manager - INFO - Detected 3 communities
2025-11-26 01:12:16,518 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:12:17,073 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:12:21,610 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:12:22,615 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:12:26,164 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:12:26,903 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:12:26,905 - src.document_processing.indexing_pipeline - INFO - Created 3 high-level nodes and 3 overview nodes
2025-11-26 01:12:26,923 - api_service - INFO - ✅ Webhook sent successfully: phase3_started
2025-11-26 01:12:26,924 - api_service - INFO - 🔄 Phase 3: Embedding Generation
2025-11-26 01:12:26,924 - src.document_processing.indexing_pipeline - INFO - Starting Phase III: Embedding Generation
2025-11-26 01:12:26,926 - src.vector.hnsw_service - INFO - HNSW service initialized: dim=1536, space=cosine
2025-11-26 01:12:26,936 - src.vector.hnsw_service - INFO - HNSW index loaded from data/processed/hnsw_index, 170 elements
2025-11-26 01:12:26,936 - src.document_processing.indexing_pipeline - INFO - Loaded existing HNSW index with 170 vectors
2025-11-26 01:12:26,936 - src.document_processing.indexing_pipeline - INFO - HNSW service initialized: True
2025-11-26 01:12:26,937 - src.document_processing.indexing_pipeline - INFO - Generating embeddings for 23 nodes
2025-11-26 01:12:27,673 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:12:28,683 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 1/2
2025-11-26 01:12:30,334 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:12:31,129 - src.vector.hnsw_service - WARNING - Node H_community_0 already exists in index
2025-11-26 01:12:31,129 - src.document_processing.indexing_pipeline - WARNING - Failed to add node H_community_0 to HNSW index
2025-11-26 01:12:31,129 - src.vector.hnsw_service - WARNING - Node H_community_1 already exists in index
2025-11-26 01:12:31,129 - src.document_processing.indexing_pipeline - WARNING - Failed to add node H_community_1 to HNSW index
2025-11-26 01:12:31,129 - src.vector.hnsw_service - WARNING - Node H_community_2 already exists in index
2025-11-26 01:12:31,129 - src.document_processing.indexing_pipeline - WARNING - Failed to add node H_community_2 to HNSW index
2025-11-26 01:12:31,129 - src.vector.hnsw_service - WARNING - Node O_community_0 already exists in index
2025-11-26 01:12:31,129 - src.document_processing.indexing_pipeline - WARNING - Failed to add node O_community_0 to HNSW index
2025-11-26 01:12:31,129 - src.vector.hnsw_service - WARNING - Node O_community_1 already exists in index
2025-11-26 01:12:31,129 - src.document_processing.indexing_pipeline - WARNING - Failed to add node O_community_1 to HNSW index
2025-11-26 01:12:31,129 - src.vector.hnsw_service - WARNING - Node O_community_2 already exists in index
2025-11-26 01:12:31,129 - src.document_processing.indexing_pipeline - WARNING - Failed to add node O_community_2 to HNSW index
2025-11-26 01:12:31,129 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 2/2
2025-11-26 01:12:31,131 - src.vector.hnsw_service - INFO - HNSW index saved to data/processed/hnsw_index
2025-11-26 01:12:31,131 - src.document_processing.indexing_pipeline - INFO - HNSW index saved: True
2025-11-26 01:12:31,131 - src.document_processing.indexing_pipeline - INFO - Phase III completed: 17 embeddings generated and indexed
2025-11-26 01:12:31,131 - api_service - INFO - 💾 Storing in NeonDB
2025-11-26 01:12:31,165 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:12:31,479 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:12:31,731 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:12:32,753 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:12:33,733 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:12:33,735 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:12:33,964 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:12:34,743 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/4d51dfd0-35da-4edd-86ba-98300251884e "HTTP/1.1 200 OK"
2025-11-26 01:12:36,817 - src.storage.neon_storage - INFO - ✅ NodeRAG database tables ensured
2025-11-26 01:12:40,588 - src.storage.neon_storage - INFO - ✅ Stored graph data for file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00
2025-11-26 01:12:40,588 - src.storage.neon_storage - INFO - 📊 Found 23 nodes to store
2025-11-26 01:12:48,885 - src.storage.neon_storage - INFO - ✅ Stored 23 embeddings for file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00
2025-11-26 01:12:49,447 - api_service - INFO - ✅ Webhook sent successfully: completed
2025-11-26 01:12:49,447 - api_service - INFO - ✅ NodeRAG processing completed for file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00
2025-11-26 01:34:08,565 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:34:08,565 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:34:08,573 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:34:08,573 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:34:12,792 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'tell about coitonic...'
2025-11-26 01:34:12,874 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:34:13,308 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:34:13,566 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:34:14,626 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:34:15,751 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:34:15,756 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:34:15,757 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 01:34:15,757 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 01:34:15,851 - src.vector.hnsw_service - INFO - HNSW service initialized: dim=1536, space=cosine
2025-11-26 01:34:15,856 - src.vector.hnsw_service - INFO - HNSW index loaded from data/processed/hnsw_index, 187 elements
2025-11-26 01:34:15,856 - src.document_processing.indexing_pipeline - INFO - Loaded existing HNSW index with 187 vectors
2025-11-26 01:34:15,857 - src.search.personalized_pagerank - INFO - PPR initialized with 0 nodes, modified=True
2025-11-26 01:34:15,857 - src.search.advanced_search - INFO - Built entity lookup with 0 unique entities
2025-11-26 01:34:15,857 - src.search.advanced_search - INFO - Advanced search system initialized
2025-11-26 01:34:15,857 - src.search.advanced_search - INFO - Advanced search for query: 'tell about coitonic'
2025-11-26 01:34:16,103 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:34:16,925 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/d37f5a82-1769-4d87-8e11-dc46638f8a8f "HTTP/1.1 200 OK"
2025-11-26 01:34:17,210 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:34:17,872 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:34:17,886 - src.search.advanced_search - INFO - Search completed: 15 final nodes
2025-11-26 01:34:17,888 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:34:17] "POST /api/v1/generate-response HTTP/1.1" 200 -
2025-11-26 01:36:25,161 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'tell about the data u have...'
2025-11-26 01:36:25,162 - src.search.advanced_search - INFO - Advanced search for query: 'tell about the data u have'
2025-11-26 01:36:25,898 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:36:26,568 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:36:26,571 - src.search.advanced_search - INFO - Search completed: 15 final nodes
2025-11-26 01:36:26,573 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:36:26] "POST /api/v1/generate-response HTTP/1.1" 200 -
2025-11-26 01:39:42,412 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:39:42,412 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:39:42,417 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:39:42,417 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:39:54,238 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'tell about coitonic...'
2025-11-26 01:39:54,277 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:39:54,623 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:39:54,879 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:39:55,971 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:39:56,979 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:39:56,981 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:39:56,982 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 01:39:56,982 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 01:39:57,074 - src.vector.hnsw_service - INFO - HNSW service initialized: dim=1536, space=cosine
2025-11-26 01:39:57,079 - src.vector.hnsw_service - INFO - HNSW index loaded from data/processed/hnsw_index, 187 elements
2025-11-26 01:39:57,079 - src.document_processing.indexing_pipeline - INFO - Loaded existing HNSW index with 187 vectors
2025-11-26 01:39:57,081 - src.search.personalized_pagerank - INFO - PPR initialized with 0 nodes, modified=True
2025-11-26 01:39:57,081 - src.search.advanced_search - INFO - Built entity lookup with 0 unique entities
2025-11-26 01:39:57,081 - src.search.advanced_search - INFO - Advanced search system initialized
2025-11-26 01:39:57,081 - api_service - ERROR - ❌ Response generation error: Connectivity is undefined for the null graph.
2025-11-26 01:39:57,082 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:39:57] "[35m[1mPOST /api/v1/generate-response HTTP/1.1[0m" 500 -
2025-11-26 01:39:57,216 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:39:57,966 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/e37051a5-c238-4047-9e8b-a272e847a5d8 "HTTP/1.1 200 OK"
2025-11-26 01:40:43,776 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:40:43,776 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:40:43,783 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:40:43,783 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:40:50,255 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'tell about the data u have...'
2025-11-26 01:40:50,292 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:40:50,660 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:40:50,919 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:40:51,959 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:40:52,925 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:40:52,928 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:40:52,929 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 01:40:52,931 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 01:40:53,020 - api_service - ERROR - ❌ Response generation error: Connectivity is undefined for the null graph.
2025-11-26 01:40:53,025 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:40:53] "[35m[1mPOST /api/v1/generate-response HTTP/1.1[0m" 500 -
2025-11-26 01:40:53,165 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:40:53,916 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/66d29a1e-7286-4bb2-8026-cc520af9d76d "HTTP/1.1 200 OK"
2025-11-26 01:41:39,195 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:41:39,195 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:41:39,200 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:41:39,200 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:41:45,384 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'hi...'
2025-11-26 01:41:45,421 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:41:45,772 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:41:46,022 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:41:47,087 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:41:48,072 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:41:48,074 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:41:48,075 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 01:41:48,075 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 01:41:48,167 - api_service - INFO - 📊 No data in memory graph, using Neon storage search
2025-11-26 01:41:48,804 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:41:49,789 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:41:50,014 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/249d2248-7263-430d-9673-2302bef772d9 "HTTP/1.1 200 OK"
2025-11-26 01:41:50,037 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:41:50,755 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:41:51,732 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:41:52,720 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:41:52,721 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:41:53,360 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:41:53,696 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/a581bea2-0d9b-47d0-9d22-56ad4f1ccfa7 "HTTP/1.1 200 OK"
2025-11-26 01:41:53,796 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:41:56,212 - src.storage.neon_storage - ERROR - ❌ Search error: syntax error at or near "2"
2025-11-26 01:41:56,215 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:41:56] "POST /api/v1/generate-response HTTP/1.1" 200 -
2025-11-26 01:42:47,285 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:42:47,286 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:42:47,294 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:42:47,294 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:42:51,929 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'tell about the data u have...'
2025-11-26 01:42:51,977 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:42:52,315 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:42:52,885 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:42:53,878 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:42:54,858 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:42:54,862 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:42:54,862 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 01:42:54,863 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 01:42:54,953 - api_service - INFO - 📊 No data in memory graph, using Neon storage search
2025-11-26 01:42:54,978 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:42:55,090 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:42:55,290 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:42:55,540 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:42:55,886 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/20c5a726-b41e-4663-ad8d-a4b6cbd1ba7a "HTTP/1.1 200 OK"
2025-11-26 01:42:56,532 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:42:57,488 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:42:57,489 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:42:57,714 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:42:58,495 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/6e1a33bb-f544-4f80-ab73-f796c3bbb916 "HTTP/1.1 200 OK"
2025-11-26 01:42:59,287 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:43:03,308 - src.storage.neon_storage - INFO - 🔍 Found 20 search results for query: 'tell about the data u have...'
2025-11-26 01:43:06,598 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:43:07,980 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:43:07,981 - api_service - INFO - ✅ Generated Neon storage response with 20 sources
2025-11-26 01:43:07,983 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:43:07] "POST /api/v1/generate-response HTTP/1.1" 200 -
2025-11-26 01:46:45,019 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:46:45] "[33mPOST /api/v1/inspect-data HTTP/1.1[0m" 404 -
2025-11-26 01:46:51,949 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:46:51,949 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:46:51,956 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:46:51,956 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:46:53,343 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:46:53,730 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:46:53,980 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:46:54,991 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:46:56,002 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:46:56,004 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:46:56,005 - api_service - ERROR - ❌ Inspect error: name 'asyncio' is not defined
2025-11-26 01:46:56,012 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:46:56] "[35m[1mPOST /api/v1/inspect-data HTTP/1.1[0m" 500 -
2025-11-26 01:46:56,235 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:46:56,989 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/23b1613f-7f79-4887-b835-529f4dcac171 "HTTP/1.1 200 OK"
2025-11-26 01:47:07,089 - api_service - ERROR - ❌ Inspect error: name 'asyncio' is not defined
2025-11-26 01:47:07,090 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:47:07] "[35m[1mPOST /api/v1/inspect-data HTTP/1.1[0m" 500 -
2025-11-26 01:47:13,632 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:47:13,632 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:47:13,638 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:47:13,638 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:47:29,104 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:47:29] "[33mGET /api/v1/debug-db HTTP/1.1[0m" 404 -
2025-11-26 01:48:05,045 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:48:05,045 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:48:05,052 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:48:05,052 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:48:06,500 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:48:06,836 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:48:07,081 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:48:08,067 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:48:09,026 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:48:09,029 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:48:09,313 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:48:10,016 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/705636cc-3542-4260-b28f-ae4c1e9ae804 "HTTP/1.1 200 OK"
2025-11-26 01:48:11,418 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:48:11] "GET /api/v1/debug-db HTTP/1.1" 200 -
2025-11-26 01:48:51,891 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'what data do you have about companies...'
2025-11-26 01:48:51,897 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:48:52,186 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:48:52,437 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:48:53,457 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:48:54,427 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:48:54,432 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:48:54,434 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 01:48:54,435 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 01:48:54,522 - api_service - INFO - 📊 No data in memory graph, using Neon storage search
2025-11-26 01:48:54,663 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:48:55,440 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/285d75b7-e51f-4d28-9c94-f56c0ddd1294 "HTTP/1.1 200 OK"
2025-11-26 01:48:55,627 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:48:59,387 - src.storage.neon_storage - INFO - 🔍 Found 20 search results for query: 'what data do you have about companies...'
2025-11-26 01:48:59,630 - api_service - INFO - 🔍 Processing 20 search results:
2025-11-26 01:48:59,630 - api_service - INFO -   Result 0: node_type=H, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=1. Concept: Coitonic Affiliate Code
Description: The text data mentions the concept of a Coitonic Af...
2025-11-26 01:48:59,631 - api_service - INFO -   Result 1: node_type=O, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=Coitonic Affiliate Program: Driving Success in Digital Marketing...
2025-11-26 01:48:59,631 - api_service - INFO -   Result 2: node_type=A, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=The Coitonic Affiliate is a dynamic and innovative partner in the realm of affiliate marketing. With...
2025-11-26 01:48:59,632 - api_service - INFO - 📝 Built context with 6227 characters from 20 nodes
2025-11-26 01:48:59,632 - api_service - INFO - 🤖 Generating answer with prompt length: 7146 characters
2025-11-26 01:48:59,632 - api_service - INFO - 📋 First 500 chars of prompt: 
You are a helpful AI assistant that ONLY uses the provided retrieved information to answer questions. Do NOT use any external knowledge or information not explicitly provided below.

IMPORTANT INSTRUCTIONS:
1. ONLY use information from the "Retrieved Information" section below
2. If the retrieved information doesn't contain enough detail to answer the question, say "I don't have enough information in the provided context to answer this question completely."
3. Do NOT make up or infer informatio...
2025-11-26 01:49:00,261 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:49:00,272 - api_service - INFO - ✅ Generated response length: 91 characters
2025-11-26 01:49:00,272 - api_service - INFO - 📄 Response preview: I don't have enough information in the provided context to answer this question completely....
2025-11-26 01:49:00,272 - api_service - INFO - ✅ Generated Neon storage response with 20 sources
2025-11-26 01:49:00,273 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:49:00] "POST /api/v1/generate-response HTTP/1.1" 200 -
2025-11-26 01:49:32,689 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'tell me about Coitonic...'
2025-11-26 01:49:32,689 - api_service - INFO - 📊 No data in memory graph, using Neon storage search
2025-11-26 01:49:33,169 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:49:36,759 - src.storage.neon_storage - INFO - 🔍 Found 20 search results for query: 'tell me about Coitonic...'
2025-11-26 01:49:36,994 - api_service - INFO - 🔍 Processing 20 search results:
2025-11-26 01:49:36,994 - api_service - INFO -   Result 0: node_type=N, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=Coitonic...
2025-11-26 01:49:36,994 - api_service - INFO -   Result 1: node_type=N, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=Coitonic brand...
2025-11-26 01:49:36,994 - api_service - INFO -   Result 2: node_type=N, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=Coitonic Affiliate...
2025-11-26 01:49:36,994 - api_service - INFO - 📝 Built context with 6232 characters from 20 nodes
2025-11-26 01:49:36,995 - api_service - INFO - 🤖 Generating answer with prompt length: 7136 characters
2025-11-26 01:49:36,995 - api_service - INFO - 📋 First 500 chars of prompt: 
You are a helpful AI assistant that ONLY uses the provided retrieved information to answer questions. Do NOT use any external knowledge or information not explicitly provided below.

IMPORTANT INSTRUCTIONS:
1. ONLY use information from the "Retrieved Information" section below
2. If the retrieved information doesn't contain enough detail to answer the question, say "I don't have enough information in the provided context to answer this question completely."
3. Do NOT make up or infer informatio...
2025-11-26 01:49:42,282 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:49:42,291 - api_service - INFO - ✅ Generated response length: 979 characters
2025-11-26 01:49:42,291 - api_service - INFO - 📄 Response preview: Based on the provided information, Coitonic is a brand that operates in the realm of affiliate marketing. They have an Affiliate Program that focuses on driving traffic and conversions for various bra...
2025-11-26 01:49:42,292 - api_service - INFO - ✅ Generated Neon storage response with 20 sources
2025-11-26 01:49:42,292 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:49:42] "POST /api/v1/generate-response HTTP/1.1" 200 -
2025-11-26 01:52:20,756 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'tell about the data u have...'
2025-11-26 01:52:20,757 - api_service - INFO - 📊 No data in memory graph, using Neon storage search
2025-11-26 01:52:21,701 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:52:25,507 - src.storage.neon_storage - INFO - 🔍 Found 20 search results for query: 'tell about the data u have...'
2025-11-26 01:52:25,756 - api_service - INFO - 🔍 Processing 20 search results:
2025-11-26 01:52:25,756 - api_service - INFO -   Result 0: node_type=N, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=XYZ...
2025-11-26 01:52:25,756 - api_service - INFO -   Result 1: node_type=H, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=1. Concept: Coitonic Affiliate Code
Description: The text data mentions the concept of a Coitonic Af...
2025-11-26 01:52:25,756 - api_service - INFO -   Result 2: node_type=S, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=Affiliate code must end with "10"...
2025-11-26 01:52:25,757 - api_service - INFO - 📝 Built context with 6167 characters from 20 nodes
2025-11-26 01:52:25,757 - api_service - INFO - 🤖 Generating answer with prompt length: 7075 characters
2025-11-26 01:52:25,757 - api_service - INFO - 📋 First 500 chars of prompt: 
You are a helpful AI assistant that ONLY uses the provided retrieved information to answer questions. Do NOT use any external knowledge or information not explicitly provided below.

IMPORTANT INSTRUCTIONS:
1. ONLY use information from the "Retrieved Information" section below
2. If the retrieved information doesn't contain enough detail to answer the question, say "I don't have enough information in the provided context to answer this question completely."
3. Do NOT make up or infer informatio...
2025-11-26 01:52:28,097 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:52:28,099 - api_service - INFO - ✅ Generated response length: 975 characters
2025-11-26 01:52:28,099 - api_service - INFO - 📄 Response preview: In the provided data, there is information about the Coitonic Affiliate Program, which is a strategic initiative focused on driving traffic and conversions for various brands through affiliate marketi...
2025-11-26 01:52:28,099 - api_service - INFO - ✅ Generated Neon storage response with 20 sources
2025-11-26 01:52:28,100 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:52:28] "POST /api/v1/generate-response HTTP/1.1" 200 -
2025-11-26 01:52:31,353 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:52:31,353 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:52:31,359 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:52:31,359 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:52:39,402 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'tell about the data u have...'
2025-11-26 01:52:39,403 - api_service - INFO - 🧠 Detected knowledge discovery query - using agentic exploration
2025-11-26 01:52:39,429 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:52:39,785 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:52:40,028 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:52:41,048 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:52:42,040 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:52:42,042 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:52:42,042 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 01:52:42,043 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 01:52:42,127 - api_service - INFO - 📊 No data in memory graph, using Neon storage search
2025-11-26 01:52:42,156 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:52:42,276 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:52:42,453 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:52:42,702 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:52:43,037 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/2196f5f8-abbc-4f98-86f3-57704a9d0074 "HTTP/1.1 200 OK"
2025-11-26 01:52:43,672 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:52:44,669 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:52:44,671 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:52:44,672 - api_service - INFO - 🔍 Performing agentic knowledge exploration
2025-11-26 01:52:44,902 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:52:45,668 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/b0a04c2b-e643-4bd0-83bc-597028383391 "HTTP/1.1 200 OK"
2025-11-26 01:52:46,445 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:52:50,017 - src.storage.neon_storage - INFO - 🔍 Found 3 search results for query: 'overview summary main topics key information...'
2025-11-26 01:52:50,880 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:52:54,681 - src.storage.neon_storage - INFO - 🔍 Found 6 search results for query: 'companies organizations entities names...'
2025-11-26 01:52:55,671 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:52:59,601 - src.storage.neon_storage - INFO - 🔍 Found 7 search results for query: 'tell about the data u have...'
2025-11-26 01:52:59,848 - api_service - INFO - 🧠 Agentic exploration found: 3 overview + 6 entities + 7 semantic = 16 total
2025-11-26 01:52:59,848 - api_service - INFO - 🔍 Processing 16 search results:
2025-11-26 01:52:59,848 - api_service - INFO -   Result 0: node_type=H, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=1. Concept: Coitonic Affiliate Code
Description: The text data mentions the concept of a Coitonic Af...
2025-11-26 01:52:59,848 - api_service - INFO -   Result 1: node_type=H, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=1. Coitonic Affiliate Program Overview:
   - The text data outlines the process of becoming a Coiton...
2025-11-26 01:52:59,848 - api_service - INFO -   Result 2: node_type=H, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=1. Coitonic Affiliate Program Overview
   The Coitonic Affiliate Program is a strategic initiative a...
2025-11-26 01:52:59,848 - api_service - INFO - 📝 Built context with 5430 characters from 16 nodes
2025-11-26 01:52:59,848 - api_service - INFO - 🤖 Generating agentic answer with prompt length: 6263 characters
2025-11-26 01:53:06,788 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:53:06,794 - api_service - INFO - ✅ Generated response length: 2370 characters
2025-11-26 01:53:06,794 - api_service - INFO - 📄 Response preview: Main Topics & Themes:
- Affiliate Marketing: The primary theme revolves around affiliate marketing strategies, programs, and codes, with a focus on the Coitonic Affiliate Program and the utilization o...
2025-11-26 01:53:06,794 - api_service - INFO - ✅ Generated Neon storage response with 16 sources
2025-11-26 01:53:06,795 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:53:06] "POST /api/v1/generate-response HTTP/1.1" 200 -
2025-11-26 01:53:59,439 - __main__ - INFO - ✅ Environment check passed
2025-11-26 01:53:59,439 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 01:53:59,447 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 01:53:59,447 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 01:54:09,558 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'tell about the data u have...'
2025-11-26 01:54:09,559 - api_service - INFO - 🧠 Detected knowledge discovery query - using agentic exploration
2025-11-26 01:54:09,595 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:54:09,983 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:54:10,238 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:54:11,302 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:54:12,332 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:54:12,334 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:54:12,335 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 01:54:12,335 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 01:54:12,428 - api_service - INFO - 📊 No data in memory graph, using Neon storage search
2025-11-26 01:54:12,453 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 01:54:12,563 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:54:12,743 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 01:54:12,998 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 01:54:13,328 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/0e5836e6-e143-4f3a-ac46-a10d49050fd6 "HTTP/1.1 200 OK"
2025-11-26 01:54:14,004 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 01:54:15,015 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 01:54:15,017 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 01:54:15,018 - api_service - INFO - 🔍 Performing agentic knowledge exploration
2025-11-26 01:54:15,246 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 01:54:15,703 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:54:16,003 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/7184e5e8-18c5-4eb5-8ba6-92fea7eec1e8 "HTTP/1.1 200 OK"
2025-11-26 01:54:19,856 - src.storage.neon_storage - INFO - 🔍 Found 3 search results for query: 'overview summary main topics key information...'
2025-11-26 01:54:21,087 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:54:24,628 - src.storage.neon_storage - INFO - 🔍 Found 6 search results for query: 'companies organizations entities names...'
2025-11-26 01:54:26,368 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 01:54:30,445 - src.storage.neon_storage - INFO - 🔍 Found 7 search results for query: 'tell about the data u have...'
2025-11-26 01:54:30,728 - api_service - INFO - 🧠 Agentic exploration found: 3 overview + 6 entities + 7 semantic = 16 total
2025-11-26 01:54:30,728 - api_service - INFO - 🔍 Processing 16 search results:
2025-11-26 01:54:30,729 - api_service - INFO -   Result 0: node_type=H, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=1. Concept: Coitonic Affiliate Code
Description: The text data mentions the concept of a Coitonic Af...
2025-11-26 01:54:30,729 - api_service - INFO -   Result 1: node_type=H, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=1. Coitonic Affiliate Program Overview:
   - The text data outlines the process of becoming a Coiton...
2025-11-26 01:54:30,729 - api_service - INFO -   Result 2: node_type=H, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=1. Coitonic Affiliate Program Overview
   The Coitonic Affiliate Program is a strategic initiative a...
2025-11-26 01:54:30,729 - api_service - INFO - 📝 Built context with 5430 characters from 16 nodes
2025-11-26 01:54:30,729 - api_service - INFO - 🤖 Generating agentic answer with prompt length: 6263 characters
2025-11-26 01:54:37,756 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 01:54:37,771 - api_service - INFO - ✅ Generated response length: 1482 characters
2025-11-26 01:54:37,771 - api_service - INFO - 📄 Response preview: Main Topics & Themes:
- The primary subjects covered in the knowledge base revolve around the Coitonic Affiliate Program, affiliate marketing strategies, affiliate codes, sponsorship programs, and dig...
2025-11-26 01:54:37,772 - api_service - INFO - ✅ Generated Neon storage response with 16 sources
2025-11-26 01:54:37,775 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 01:54:37] "POST /api/v1/generate-response HTTP/1.1" 200 -
2025-11-26 02:37:37,650 - __main__ - INFO - ✅ Environment check passed
2025-11-26 02:37:37,650 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 02:37:37,659 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 02:37:37,659 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 02:37:47,886 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'hi...'
2025-11-26 02:37:47,921 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 02:37:49,058 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 02:37:49,318 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 02:37:50,500 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 02:37:51,469 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 02:37:51,471 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 02:37:51,473 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 02:37:51,473 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 02:37:51,567 - api_service - INFO - 📊 No data in memory graph, using Neon storage search
2025-11-26 02:37:51,591 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 02:37:51,708 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 02:37:51,922 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 02:37:52,185 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 02:37:52,524 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/582f2bc0-c2e2-4039-a624-642b30b76320 "HTTP/1.1 200 OK"
2025-11-26 02:37:53,171 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 02:37:54,171 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 02:37:54,174 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 02:37:54,410 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 02:37:55,186 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/f9bb38d2-5fc2-49e3-b68c-daa3583d7df7 "HTTP/1.1 200 OK"
2025-11-26 02:37:55,355 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2025-11-26 02:37:59,209 - src.storage.neon_storage - INFO - 🔍 Found 20 search results for query: 'hi...'
2025-11-26 02:37:59,461 - api_service - INFO - 🔍 Processing 20 search results:
2025-11-26 02:37:59,461 - api_service - INFO -   Result 0: node_type=N, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=XYZ...
2025-11-26 02:37:59,461 - api_service - INFO -   Result 1: node_type=S, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=Contact information: Email - info@coitonic.com, Instagram - @coitonic, Website - www.coitonic.com...
2025-11-26 02:37:59,461 - api_service - INFO -   Result 2: node_type=N, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=Coitonic Affiliate...
2025-11-26 02:37:59,461 - api_service - INFO - 📝 Built context with 4835 characters from 20 nodes
2025-11-26 02:37:59,462 - api_service - INFO - 🤖 Generating standard answer with prompt length: 5719 characters
2025-11-26 02:38:00,210 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-26 02:38:00,228 - api_service - INFO - ✅ Generated response length: 69 characters
2025-11-26 02:38:00,229 - api_service - INFO - 📄 Response preview: Hello! How can I assist you today regarding the information provided?...
2025-11-26 02:38:00,522 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 02:38:00,522 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-11-26 02:38:00,523 - api_service - ERROR - ❌ Response generation error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-11-26 02:38:00,524 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 02:38:00] "[35m[1mPOST /api/v1/generate-response HTTP/1.1[0m" 500 -
2025-11-26 02:40:09,864 - __main__ - INFO - ✅ Environment check passed
2025-11-26 02:40:09,864 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 02:40:09,870 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 02:40:09,870 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 02:40:15,357 - api_service - INFO - 🤖 Generating NodeRAG response for query: 'hi...'
2025-11-26 02:40:15,388 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 02:40:15,886 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 02:40:16,141 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 02:40:17,277 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 02:40:18,316 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 02:40:18,319 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 02:40:18,320 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 02:40:18,321 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 02:40:18,409 - api_service - INFO - 📊 No data in memory graph, using Neon storage search
2025-11-26 02:40:18,431 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 02:40:18,556 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 02:40:18,853 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 02:40:19,108 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 02:40:19,352 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/c088ab44-4335-42fa-9779-020535bba4cd "HTTP/1.1 200 OK"
2025-11-26 02:40:20,137 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 02:40:21,146 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 02:40:21,149 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 02:40:21,384 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 02:40:21,820 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 401 Unauthorized"
2025-11-26 02:40:21,823 - src.llm.llm_service - ERROR - Error getting embeddings: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 02:40:22,166 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/f2fed1fe-a433-4634-8ee8-94d5ab2609b5 "HTTP/1.1 200 OK"
2025-11-26 02:40:25,716 - src.storage.neon_storage - INFO - 🔍 Found 20 search results for query: 'hi...'
2025-11-26 02:40:25,973 - api_service - INFO - 🔍 Processing 20 search results:
2025-11-26 02:40:25,973 - api_service - INFO -   Result 0: node_type=O, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=Coitonic Affiliate Program: Driving Success in Digital Marketing...
2025-11-26 02:40:25,974 - api_service - INFO -   Result 1: node_type=H, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=1. Coitonic Affiliate Program Overview
   The Coitonic Affiliate Program is a strategic initiative a...
2025-11-26 02:40:25,974 - api_service - INFO -   Result 2: node_type=N, file_id=f8775e45-2f02-4363-a44b-c6af96ca6a00, content_preview=XYZ...
2025-11-26 02:40:25,974 - api_service - INFO - 📝 Built context with 5765 characters from 20 nodes
2025-11-26 02:40:25,974 - api_service - INFO - 🤖 Generating standard answer with prompt length: 6649 characters
2025-11-26 02:40:26,358 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 02:40:26,359 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-11-26 02:40:26,359 - api_service - ERROR - ❌ Response generation error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-11-26 02:40:26,364 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 02:40:26] "[35m[1mPOST /api/v1/generate-response HTTP/1.1[0m" 500 -
2025-11-26 04:26:04,492 - __main__ - INFO - ✅ Environment check passed
2025-11-26 04:26:04,492 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 04:26:04,501 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 04:26:04,501 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 04:27:05,572 - api_service - INFO - 🚀 Starting NodeRAG processing: file_id=e8d9d2f8-5894-45cc-8087-4a6166a96d28, org_id=12aff77d-e387-4f4b-93bd-b294756dd96f, chunks=1
2025-11-26 04:27:05,613 - api_service - INFO - ✅ Webhook sent successfully: processing
2025-11-26 04:27:05,624 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 04:27:05] "[35m[1mPOST /api/v1/process-document HTTP/1.1[0m" 202 -
2025-11-26 04:27:05,704 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 04:27:06,176 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 04:27:06,436 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 04:27:07,484 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 04:27:08,469 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 04:27:08,472 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 04:27:08,474 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 04:27:08,474 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 04:27:08,562 - api_service - INFO - ✅ Webhook sent successfully: phase1_started
2025-11-26 04:27:08,562 - api_service - INFO - 🔄 Phase 1: Graph Decomposition
2025-11-26 04:27:08,562 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-26 04:27:08,708 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 04:27:09,084 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:27:09,086 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:27:09,087 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:27:09,374 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:27:09,375 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:27:09,375 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:27:09,375 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 1 successful, 0 failed
2025-11-26 04:27:09,387 - api_service - INFO - ✅ Webhook sent successfully: phase2_started
2025-11-26 04:27:09,387 - api_service - INFO - 🔄 Phase 2: Graph Augmentation
2025-11-26 04:27:09,387 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-26 04:27:09,388 - src.document_processing.indexing_pipeline - INFO - Identified 0 important entities
2025-11-26 04:27:09,388 - src.document_processing.indexing_pipeline - INFO - Created 0 attribute nodes
2025-11-26 04:27:09,420 - src.graph.graph_manager - INFO - Detected 1 communities
2025-11-26 04:27:09,501 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/fdb2e64b-2a1f-429d-8ba9-9ffd0622c33f "HTTP/1.1 200 OK"
2025-11-26 04:27:09,709 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:27:09,710 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:27:09,710 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:27:09,998 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:27:09,999 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:27:10,000 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:27:10,000 - src.document_processing.indexing_pipeline - INFO - Created 1 high-level nodes and 1 overview nodes
2025-11-26 04:27:10,015 - api_service - INFO - ✅ Webhook sent successfully: phase3_started
2025-11-26 04:27:10,015 - api_service - INFO - 🔄 Phase 3: Embedding Generation
2025-11-26 04:27:10,015 - src.document_processing.indexing_pipeline - INFO - Starting Phase III: Embedding Generation
2025-11-26 04:27:10,017 - src.vector.hnsw_service - INFO - HNSW service initialized: dim=1536, space=cosine
2025-11-26 04:27:10,025 - src.vector.hnsw_service - INFO - HNSW index loaded from data/processed/hnsw_index, 187 elements
2025-11-26 04:27:10,025 - src.document_processing.indexing_pipeline - INFO - Loaded existing HNSW index with 187 vectors
2025-11-26 04:27:10,025 - src.document_processing.indexing_pipeline - INFO - HNSW service initialized: True
2025-11-26 04:27:10,025 - src.document_processing.indexing_pipeline - INFO - Generating embeddings for 2 nodes
2025-11-26 04:27:10,355 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 401 Unauthorized"
2025-11-26 04:27:10,356 - src.llm.llm_service - ERROR - Error getting embeddings: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:27:10,356 - src.vector.hnsw_service - WARNING - Node H_community_0 already exists in index
2025-11-26 04:27:10,356 - src.document_processing.indexing_pipeline - WARNING - Failed to add node H_community_0 to HNSW index
2025-11-26 04:27:10,356 - src.vector.hnsw_service - WARNING - Node O_community_0 already exists in index
2025-11-26 04:27:10,356 - src.document_processing.indexing_pipeline - WARNING - Failed to add node O_community_0 to HNSW index
2025-11-26 04:27:10,356 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 1/1
2025-11-26 04:27:10,361 - src.vector.hnsw_service - INFO - HNSW index saved to data/processed/hnsw_index
2025-11-26 04:27:10,361 - src.document_processing.indexing_pipeline - INFO - HNSW index saved: True
2025-11-26 04:27:10,361 - src.document_processing.indexing_pipeline - INFO - Phase III completed: 0 embeddings generated and indexed
2025-11-26 04:27:10,361 - api_service - INFO - 💾 Storing in NeonDB
2025-11-26 04:27:10,404 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 04:27:10,658 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 04:27:10,910 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 04:27:11,965 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 04:27:12,970 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 04:27:12,973 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 04:27:13,209 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 04:27:13,944 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/d9fd5928-a89c-43a5-87fa-18b0404e3ea0 "HTTP/1.1 200 OK"
2025-11-26 04:27:15,978 - src.storage.neon_storage - INFO - ✅ NodeRAG database tables ensured
2025-11-26 04:27:18,860 - src.storage.neon_storage - INFO - ✅ Stored graph data for file_id=e8d9d2f8-5894-45cc-8087-4a6166a96d28
2025-11-26 04:27:18,861 - src.storage.neon_storage - INFO - 📊 Found 2 nodes to store
2025-11-26 04:27:21,650 - src.storage.neon_storage - INFO - ✅ Stored 2 embeddings for file_id=e8d9d2f8-5894-45cc-8087-4a6166a96d28
2025-11-26 04:27:22,179 - api_service - INFO - ✅ Webhook sent successfully: completed
2025-11-26 04:27:22,180 - api_service - INFO - ✅ NodeRAG processing completed for file_id=e8d9d2f8-5894-45cc-8087-4a6166a96d28
2025-11-26 04:36:24,316 - __main__ - INFO - ✅ Environment check passed
2025-11-26 04:36:24,316 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-26 04:36:24,323 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-26 04:36:24,323 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-26 04:39:05,669 - api_service - INFO - 🚀 Starting NodeRAG processing: file_id=ade9b512-6c96-4f8e-b615-d9f190012147, org_id=12aff77d-e387-4f4b-93bd-b294756dd96f, chunks=2
2025-11-26 04:39:06,213 - api_service - INFO - ✅ Webhook sent successfully: processing
2025-11-26 04:39:06,219 - werkzeug - INFO - 127.0.0.1 - - [26/Nov/2025 04:39:06] "[35m[1mPOST /api/v1/process-document HTTP/1.1[0m" 202 -
2025-11-26 04:39:06,258 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 04:39:06,610 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 04:39:06,857 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 04:39:07,915 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 04:39:08,886 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 04:39:08,888 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 04:39:08,888 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-26 04:39:08,888 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-26 04:39:09,117 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 04:39:09,231 - api_service - INFO - ✅ Webhook sent successfully: phase1_started
2025-11-26 04:39:09,232 - api_service - INFO - 🔄 Phase 1: Graph Decomposition
2025-11-26 04:39:09,232 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition
2025-11-26 04:39:09,897 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/b4f03e12-381e-4d5f-bd3e-840dc5f8830b "HTTP/1.1 200 OK"
2025-11-26 04:39:11,201 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:39:11,203 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:11,204 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:11,499 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:39:11,500 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:11,500 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:11,789 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:39:11,789 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:11,789 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:12,077 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:39:12,077 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:12,077 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:12,077 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 2 successful, 0 failed
2025-11-26 04:39:12,350 - api_service - INFO - ✅ Webhook sent successfully: phase2_started
2025-11-26 04:39:12,352 - api_service - INFO - 🔄 Phase 2: Graph Augmentation
2025-11-26 04:39:12,352 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation
2025-11-26 04:39:12,356 - src.document_processing.indexing_pipeline - INFO - Identified 0 important entities
2025-11-26 04:39:12,357 - src.document_processing.indexing_pipeline - INFO - Created 0 attribute nodes
2025-11-26 04:39:12,364 - src.graph.graph_manager - INFO - Detected 2 communities
2025-11-26 04:39:12,657 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:39:12,657 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:12,657 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:12,945 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:39:12,947 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:12,947 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:13,242 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:39:13,243 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-11-26 04:39:13,243 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-11-26 04:39:13,881 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-11-26 04:39:13,881 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:13,881 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:13,881 - src.document_processing.indexing_pipeline - INFO - Created 2 high-level nodes and 2 overview nodes
2025-11-26 04:39:14,150 - api_service - INFO - ✅ Webhook sent successfully: phase3_started
2025-11-26 04:39:14,152 - api_service - INFO - 🔄 Phase 3: Embedding Generation
2025-11-26 04:39:14,152 - src.document_processing.indexing_pipeline - INFO - Starting Phase III: Embedding Generation
2025-11-26 04:39:14,160 - src.vector.hnsw_service - INFO - HNSW service initialized: dim=1536, space=cosine
2025-11-26 04:39:14,166 - src.vector.hnsw_service - INFO - HNSW index loaded from data/processed/hnsw_index, 187 elements
2025-11-26 04:39:14,166 - src.document_processing.indexing_pipeline - INFO - Loaded existing HNSW index with 187 vectors
2025-11-26 04:39:14,166 - src.document_processing.indexing_pipeline - INFO - HNSW service initialized: True
2025-11-26 04:39:14,167 - src.document_processing.indexing_pipeline - INFO - Generating embeddings for 4 nodes
2025-11-26 04:39:15,631 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 401 Unauthorized"
2025-11-26 04:39:15,633 - src.llm.llm_service - ERROR - Error getting embeddings: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************-iUA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}
2025-11-26 04:39:15,633 - src.vector.hnsw_service - WARNING - Node H_community_0 already exists in index
2025-11-26 04:39:15,634 - src.document_processing.indexing_pipeline - WARNING - Failed to add node H_community_0 to HNSW index
2025-11-26 04:39:15,634 - src.vector.hnsw_service - WARNING - Node H_community_1 already exists in index
2025-11-26 04:39:15,634 - src.document_processing.indexing_pipeline - WARNING - Failed to add node H_community_1 to HNSW index
2025-11-26 04:39:15,634 - src.vector.hnsw_service - WARNING - Node O_community_0 already exists in index
2025-11-26 04:39:15,634 - src.document_processing.indexing_pipeline - WARNING - Failed to add node O_community_0 to HNSW index
2025-11-26 04:39:15,634 - src.vector.hnsw_service - WARNING - Node O_community_1 already exists in index
2025-11-26 04:39:15,634 - src.document_processing.indexing_pipeline - WARNING - Failed to add node O_community_1 to HNSW index
2025-11-26 04:39:15,634 - src.document_processing.indexing_pipeline - INFO - Generated embeddings for batch 1/1
2025-11-26 04:39:15,646 - src.vector.hnsw_service - INFO - HNSW index saved to data/processed/hnsw_index
2025-11-26 04:39:15,646 - src.document_processing.indexing_pipeline - INFO - HNSW index saved: True
2025-11-26 04:39:15,646 - src.document_processing.indexing_pipeline - INFO - Phase III completed: 0 embeddings generated and indexed
2025-11-26 04:39:15,646 - api_service - INFO - 💾 Storing in NeonDB
2025-11-26 04:39:15,680 - src.llm.llm_service - INFO - OpenAI client initialized successfully
2025-11-26 04:39:15,974 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-26 04:39:16,218 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-26 04:39:17,188 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-26 04:39:18,150 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-26 04:39:18,153 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-26 04:39:18,383 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-26 04:39:19,132 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/41f30ac9-1f6b-480e-9523-f5631997f96d "HTTP/1.1 200 OK"
2025-11-26 04:39:21,272 - src.storage.neon_storage - INFO - ✅ NodeRAG database tables ensured
2025-11-26 04:39:24,232 - src.storage.neon_storage - INFO - ✅ Stored graph data for file_id=ade9b512-6c96-4f8e-b615-d9f190012147
2025-11-26 04:39:24,233 - src.storage.neon_storage - INFO - 📊 Found 4 nodes to store
2025-11-26 04:39:27,413 - src.storage.neon_storage - INFO - ✅ Stored 4 embeddings for file_id=ade9b512-6c96-4f8e-b615-d9f190012147
2025-11-26 04:39:28,452 - api_service - INFO - ✅ Webhook sent successfully: completed
2025-11-26 04:39:28,453 - api_service - INFO - ✅ NodeRAG processing completed for file_id=ade9b512-6c96-4f8e-b615-d9f190012147
2025-11-27 23:08:48,595 - __main__ - INFO - ✅ Environment check passed
2025-11-27 23:08:48,596 - __main__ - INFO - 🌐 Starting server on 0.0.0.0:5001 (debug=False)
2025-11-27 23:08:48,604 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.29.147:5001
2025-11-27 23:08:48,604 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-11-27 23:12:20,377 - api_service - INFO - 🚀 Starting NodeRAG processing: file_id=cf80688f-8b39-4475-9073-1c7a382c8d5f, org_id=12aff77d-e387-4f4b-93bd-b294756dd96f, chunks=68
2025-11-27 23:12:20,975 - api_service - INFO - ✅ Webhook sent successfully: processing
2025-11-27 23:12:20,982 - werkzeug - INFO - 127.0.0.1 - - [27/Nov/2025 23:12:20] "[35m[1mPOST /api/v1/process-document HTTP/1.1[0m" 202 -
2025-11-27 23:12:21,030 - src.llm.llm_service - INFO - OpenAI clients initialized successfully
2025-11-27 23:12:21,375 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings "HTTP/1.1 200 OK"
2025-11-27 23:12:21,630 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/spaces/mahendraVarmaGokaraju/qwen3-embeddings/runtime "HTTP/1.1 200 OK"
2025-11-27 23:12:22,725 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/config "HTTP/1.1 200 OK"
2025-11-27 23:12:23,921 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/info?serialize=False "HTTP/1.1 200 OK"
2025-11-27 23:12:23,922 - src.llm.llm_service - INFO - HF embedding client initialized: mahendraVarmaGokaraju/qwen3-embeddings
2025-11-27 23:12:23,923 - src.document_processing.llamaparse_service - INFO - LlamaParse service initialized
2025-11-27 23:12:23,924 - src.document_processing.llamaparse_service - INFO - LlamaParse integration enabled
2025-11-27 23:12:24,150 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/py_client/initiated "HTTP/1.1 200 OK"
2025-11-27 23:12:24,313 - api_service - INFO - ✅ Webhook sent successfully: phase1_started
2025-11-27 23:12:24,315 - api_service - INFO - 🔄 Phase 1: Graph Decomposition
2025-11-27 23:12:24,316 - src.document_processing.indexing_pipeline - INFO - Starting Phase I: Graph Decomposition (Parallel)
2025-11-27 23:12:25,081 - httpx - INFO - HTTP Request: GET https://mahendravarmagokaraju-qwen3-embeddings.hf.space/gradio_api/heartbeat/26bdf1a7-6add-41e9-bea1-7dcded67a977 "HTTP/1.1 200 OK"
2025-11-27 23:12:25,312 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:25,316 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,317 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,339 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:25,340 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,341 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,354 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:25,354 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,355 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:25,355 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,356 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,356 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,660 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:25,661 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,661 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,674 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:25,674 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:25,675 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,009 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:26,010 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,011 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,053 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:26,055 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,055 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,067 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:26,068 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,069 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,343 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:26,345 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,345 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,383 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:26,384 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,384 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,394 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:26,395 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,395 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,395 - src.document_processing.indexing_pipeline - INFO - Processed 5/68 chunks (Success: 5, Failed: 0)
2025-11-27 23:12:26,660 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:26,661 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,661 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,717 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:26,718 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,718 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,723 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:26,724 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:26,724 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,034 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:27,035 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,036 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,055 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:27,056 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,057 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,360 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:27,362 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,362 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,412 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:27,413 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,413 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,420 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:27,431 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,432 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,695 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:27,696 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,696 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,700 - src.document_processing.indexing_pipeline - INFO - Processed 10/68 chunks (Success: 10, Failed: 0)
2025-11-27 23:12:27,730 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:27,731 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,731 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,758 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:27,759 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,759 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,881 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:27,882 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:27,882 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,077 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,078 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,078 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,081 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,082 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,082 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,092 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,093 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,093 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,211 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,211 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,211 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,405 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,406 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,406 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,425 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,425 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,426 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,449 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,449 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,449 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,449 - src.document_processing.indexing_pipeline - INFO - Processed 15/68 chunks (Success: 15, Failed: 0)
2025-11-27 23:12:28,553 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,554 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,554 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,719 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,720 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,720 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,751 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,752 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,752 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,760 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,761 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,761 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,881 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:28,883 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:28,883 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,041 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,042 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,042 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,087 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,088 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,088 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,098 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,099 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,099 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,230 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,230 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,230 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,370 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,370 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,370 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,370 - src.document_processing.indexing_pipeline - INFO - Processed 20/68 chunks (Success: 20, Failed: 0)
2025-11-27 23:12:29,435 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,436 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,436 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,454 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,455 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,455 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,548 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,548 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,548 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,683 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,684 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,684 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,755 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,756 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,756 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,785 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,786 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,786 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,878 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:29,878 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:29,878 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,003 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,004 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,004 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,085 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,087 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,087 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,185 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,186 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,187 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,215 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,216 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,216 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,216 - src.document_processing.indexing_pipeline - INFO - Processed 25/68 chunks (Success: 25, Failed: 0)
2025-11-27 23:12:30,331 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,331 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,331 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,465 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,466 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,466 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,502 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,503 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,503 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,569 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,569 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,569 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,704 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,706 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,706 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,793 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,793 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,793 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,831 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:30,831 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:30,831 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,046 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,047 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,047 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,061 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,062 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,062 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,120 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,121 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,122 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,123 - src.document_processing.indexing_pipeline - INFO - Processed 30/68 chunks (Success: 30, Failed: 0)
2025-11-27 23:12:31,171 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,172 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,172 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,361 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,363 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,363 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,404 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,404 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,404 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,486 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,486 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,486 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,495 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,495 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,495 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,687 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,689 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,689 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,752 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,753 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,753 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,842 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,843 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,845 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,858 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:31,859 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,859 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:31,859 - src.document_processing.indexing_pipeline - INFO - Processed 35/68 chunks (Success: 35, Failed: 0)
2025-11-27 23:12:32,004 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,005 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,006 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,104 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,105 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,105 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,167 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,168 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,169 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,182 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,183 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,183 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,321 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,322 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,323 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,433 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,435 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,435 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,490 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,491 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,491 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,523 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,525 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,525 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,641 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,642 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,642 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,643 - src.document_processing.indexing_pipeline - INFO - Processed 40/68 chunks (Success: 40, Failed: 0)
2025-11-27 23:12:32,771 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,772 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,773 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,825 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:32,826 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:32,826 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,056 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:33,057 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,057 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,065 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:33,066 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,066 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,182 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:33,183 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,184 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,463 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:33,464 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,464 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,489 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:33,490 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,490 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,622 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:33,623 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,623 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:33,793 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,793 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,809 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:33,810 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,810 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,977 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:33,978 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:33,978 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,134 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:34,136 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,136 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:34,136 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,137 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,137 - src.document_processing.indexing_pipeline - INFO - Processed 45/68 chunks (Success: 45, Failed: 0)
2025-11-27 23:12:34,139 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,297 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:34,299 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,299 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,491 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:34,493 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,493 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,528 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:34,529 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,529 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,618 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:34,619 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,619 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,682 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:34,683 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,683 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,815 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:34,816 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,816 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,873 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:34,874 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,874 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,940 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:34,941 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,941 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:34,941 - src.document_processing.indexing_pipeline - INFO - Processed 50/68 chunks (Success: 50, Failed: 0)
2025-11-27 23:12:35,002 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,002 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,002 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,137 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,137 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,137 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,209 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,210 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,210 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,265 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,267 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,267 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,319 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,320 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,320 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,480 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,482 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,482 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,537 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,538 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,539 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,588 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,589 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,589 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,634 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,635 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,636 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,639 - src.document_processing.indexing_pipeline - INFO - Processed 55/68 chunks (Success: 55, Failed: 0)
2025-11-27 23:12:35,826 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,828 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,828 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,906 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,906 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,906 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,960 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,961 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,961 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,965 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:35,967 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:35,967 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,152 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,153 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,154 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,225 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,226 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,227 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,287 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,288 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,289 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,289 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,290 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,292 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,484 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,485 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,485 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,557 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,557 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,557 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,606 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,607 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,607 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,610 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,611 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,611 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,802 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,803 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,804 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,804 - src.document_processing.indexing_pipeline - INFO - Processed 60/68 chunks (Success: 60, Failed: 0)
2025-11-27 23:12:36,874 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,876 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,877 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,928 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,929 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,929 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,942 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:36,942 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:36,942 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,159 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:37,161 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,161 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,194 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:37,195 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,195 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,257 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:37,258 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,258 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,267 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:37,267 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,268 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,486 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:37,487 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,488 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,520 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:37,521 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,521 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,521 - src.document_processing.indexing_pipeline - INFO - Processed 65/68 chunks (Success: 65, Failed: 0)
2025-11-27 23:12:37,615 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:37,616 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,616 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,731 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:37,732 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,732 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,852 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:37,853 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:37,853 - src.llm.llm_service - ERROR - Error extracting entities: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:38,183 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:38,185 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:38,185 - src.llm.llm_service - ERROR - Error extracting semantic units: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:38,186 - src.document_processing.indexing_pipeline - INFO - Processed 68/68 chunks (Success: 68, Failed: 0)
2025-11-27 23:12:38,187 - src.document_processing.indexing_pipeline - INFO - Phase I completed: 68 successful, 0 failed
2025-11-27 23:12:38,766 - api_service - INFO - ✅ Webhook sent successfully: phase2_started
2025-11-27 23:12:38,771 - api_service - INFO - 🔄 Phase 2: Graph Augmentation
2025-11-27 23:12:38,773 - src.document_processing.indexing_pipeline - INFO - Starting Phase II: Graph Augmentation (Parallel)
2025-11-27 23:12:38,778 - src.document_processing.indexing_pipeline - INFO - Identified 0 important entities
2025-11-27 23:12:38,778 - src.document_processing.indexing_pipeline - INFO - Created 0 attribute nodes
2025-11-27 23:12:38,785 - src.graph.graph_manager - INFO - Detected 68 communities
2025-11-27 23:12:39,109 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:39,111 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,111 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:39,112 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,113 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:39,114 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,115 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,118 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,118 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,526 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:39,527 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:39,527 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:39,528 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,529 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,529 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,529 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,530 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,537 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,867 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:39,869 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,870 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,897 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:39,898 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,898 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,913 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:39,915 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:39,915 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,206 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:40,207 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,207 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,223 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:40,224 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,225 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,239 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:40,240 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,240 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,551 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:40,552 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,552 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,563 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:40,563 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,563 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,813 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:40,814 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,815 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,876 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:40,877 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,877 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,911 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:40,912 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:40,912 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,140 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:41,141 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,141 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,207 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:41,208 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,208 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,225 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:41,226 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,226 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,471 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:41,471 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,471 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,532 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:41,532 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,532 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,545 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:41,546 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,546 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,796 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:41,797 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,797 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,866 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:41,867 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,867 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,885 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:41,888 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:41,888 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,115 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:42,116 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,116 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,191 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:42,192 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,192 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,251 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:42,251 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,251 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,443 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:42,444 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,444 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,525 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:42,526 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,526 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,571 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:42,572 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,573 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,784 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:42,785 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,786 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,877 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:42,878 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,879 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,903 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:42,903 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:42,903 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,116 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:43,117 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,118 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,197 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:43,198 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,198 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,387 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:43,387 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,388 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,524 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:43,524 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,525 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,717 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:43,718 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,719 - src.llm.llm_service - ERROR - Error generating community overview: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,853 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-11-27 23:12:43,854 - src.llm.llm_service - ERROR - OpenAI API error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-11-27 23:12:43,854 - src.llm.llm_service - ERROR - Error generating community summary: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
